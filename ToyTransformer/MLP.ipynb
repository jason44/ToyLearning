{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b762e50-ac98-4836-93d3-a4c063de0872",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "999fa2f5-d6d3-4404-9a73-8b545c63e17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# hardware acceleration\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c63434f-ffb3-4c7f-9686-b92bb5524704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already have input data\n"
     ]
    }
   ],
   "source": [
    "# get input\n",
    "if not os.path.exists('input.txt'):\n",
    "    import requests\n",
    "    data = requests.get('https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt')\n",
    "    with open('input.txt', 'w') as f:\n",
    "        f.write(data.text)\n",
    "    print('finished downloading input data')\n",
    "else:\n",
    "    print('already have input data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfabcd25-9956-4ed1-aeeb-bf816a27b9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_chars: 1115394\n"
     ]
    }
   ],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    print('n_chars:', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06d168e5-55fc-4ea9-a12e-0396cf170070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "caf02a28-a887-4d19-bf43-92583557022e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "vocab_size: 65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print('vocab_size:', vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f192b5b-a883-4f39-9b45-232517033974",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "We need to convert the raw text data as a sequence of integers according to some encoding or structure. Since this is a character level model, we just create a bijective mapping from characters to integers, but there are much more sophisticated tokenization techniques like BPE used by [OpenAi](https://github.com/openai/tiktoken), which is a sub-word tokenizer.\n",
    "\n",
    "Sub-word tokenizers are desirable because they allow the model to better understand grammer by breaking up words into chunks that are of common form. For example the tokenizer may encode '-ing' '-ed' particles into their own chunks so the model can learn the tense of a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f1f1f95-a676-4501-abce-330959382b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mapping from characters to integers\n",
    "stoi = {ch:i for i, ch in enumerate(chars)}\n",
    "itos = {i:ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s] # takes a string: outputs a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # takes a list of integers, output a string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "913ee88a-e6e4-4f6c-989e-ffa0df8b5cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode text\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "# create training and validation splits\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9961d7a-66cf-413e-8960-019778d0011a",
   "metadata": {},
   "source": [
    "### Sequential learning\n",
    "Token generation is an **auto-regressive** task. That is, we want to predict the next value in the sequence based on previous values. A predicted value is modeled as the linear combination of its previous outputs plus some noise. Clearly, words, sentences and natural language in general are auto regressive in nature.\n",
    "\n",
    "One of the benefits of training auto regressive models is that they are *self-supervised*. That is, we do not need to explicitly label our training data, as the corresponding output we want to predict is just the next value in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8d2683-9b08-4c0d-9660-351a81e0dced",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ecd7260-3058-4861-8967-769509567b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 8]) torch.Size([64, 8])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "block_size = 8 # also known as context_length\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]) # stack along dim 0\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "# why are we taking 8 outputs for each batch?\n",
    "xb, yb = get_batch('train')\n",
    "print(xb.shape, yb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b52692c-dea7-4890-9b54-b49dfd2c4abe",
   "metadata": {},
   "source": [
    "---\n",
    "### Review: Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef369c96-648a-4526-a28f-8cc252b5ab56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 65])\n",
      "tensor(4.5317, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "\n",
      ",Tc.'MPBXIvo;  c:.&PSprA'T!rWLlQLZfQs!q&-?fXtNt,Ai\n",
      "a&?Kr QHL\n",
      "Wo:p-ltOpasvYygj,qRTidgkclyamAWN:D&bgdw\n"
     ]
    }
   ],
   "source": [
    "class BigramLM(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        logits = self.token_embedding_table(idx) # [batch_size, time/block_size, channels/vocab_size] -> [B, T, C]\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            # for F.cross_entropy, we need to flatten the B and T dims\n",
    "            # there are B batches and T characters in each batch (with C=vocab_size channels)\n",
    "            # so each row in the matrix below is some channel of a character from the batch\n",
    "            logits = logits.view(B*T, C) \n",
    "            # each entry in the vector below corresponds to the target value of each character in logits[B*T, C]\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get prediction from forward()\n",
    "            logits, loss = self(idx)\n",
    "            # only interested in predicting the next character\n",
    "            logits = logits[:, -1, :] # (B, C)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "m = BigramLM(vocab_size).to(device)\n",
    "logits, loss = m(xb, yb)\n",
    "\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "# 0 corresponds to new-line\n",
    "idx = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(idx=idx, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "215d8ce1-f000-4554-8685-22ab2eb0c031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train Bigram model\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "523edab8-fbb2-43fb-adf9-ee8060512185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate loss by taking an average loss over several batches\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model, eval_iters=100):\n",
    "    out = {}\n",
    "    model.eval() # set model to eval phase\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            _, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean().item()\n",
    "    model.train() # set model to train phase\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd01db93-bb90-4452-a59c-ce068e3fcc2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/10000 : {'train': 4.576886177062988, 'val': 4.583799839019775}\n",
      "1000/10000 : {'train': 3.5614309310913086, 'val': 3.575105667114258}\n",
      "2000/10000 : {'train': 3.000180244445801, 'val': 3.01057505607605}\n",
      "3000/10000 : {'train': 2.7102270126342773, 'val': 2.731668710708618}\n",
      "4000/10000 : {'train': 2.5857350826263428, 'val': 2.6017632484436035}\n",
      "5000/10000 : {'train': 2.536057949066162, 'val': 2.5493409633636475}\n",
      "6000/10000 : {'train': 2.497814178466797, 'val': 2.516511917114258}\n",
      "7000/10000 : {'train': 2.4835495948791504, 'val': 2.4963839054107666}\n",
      "8000/10000 : {'train': 2.4720680713653564, 'val': 2.4908885955810547}\n",
      "9000/10000 : {'train': 2.4658913612365723, 'val': 2.491212844848633}\n"
     ]
    }
   ],
   "source": [
    "max_steps = 10000\n",
    "for steps in range(max_steps):\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if steps % 1000 == 0:\n",
    "        print(f\"{steps}/{max_steps} : {estimate_loss(m)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "659d2433-9ae3-48ad-95cb-2bc19c9442c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TUMy ont my hares, h heveistyldiese le thengafthan im, inkendiks th be\n",
      "QUCl he\n",
      "roct gl.\n",
      "KI' y meathe\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(idx=idx, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641b8bc3-d017-42f9-a47c-c4189d6fc873",
   "metadata": {},
   "source": [
    "---\n",
    "### Longer Context Length Model\n",
    "Pytorch implementation of what we had in MLP-3\n",
    "\n",
    "In addition, we will add a *positional embedding* layer along the *token embedding* layer. Hence, the model will also learn the positional relationship between different tokens in addition to their semantic  relationship learned by the token embedding layer. This layer will be crucial for self-attention in transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16cf2930-34bf-4a12-b557-2fcd4c079c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 8])\n",
      "torch.Size([64, 65])\n"
     ]
    }
   ],
   "source": [
    "class MLP_M(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        n_embd = 48\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.flatten = nn.Flatten(1)\n",
    "        self.layer1 = nn.Linear(block_size*n_embd, 100)\n",
    "        self.bn1 = nn.BatchNorm1d(100)\n",
    "        self.layer2 = nn.Linear(100, 200)\n",
    "        self.bn2 = nn.BatchNorm1d(200)\n",
    "        self.layer3 = nn.Linear(200, 300)\n",
    "        self.bn3 = nn.BatchNorm1d(300)\n",
    "        self.layer4 = nn.Linear(300, vocab_size)\n",
    "\n",
    "        self.relu1 = nn.LeakyReLU()\n",
    "        self.relu2 = nn.LeakyReLU()\n",
    "        self.relu3 = nn.LeakyReLU()\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        tok_emb = self.token_embedding_table(idx) \n",
    "        pos_emb = self.position_embedding_table(torch.arange(idx.shape[1], device=device))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.flatten(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu3(x)\n",
    "        logits = self.layer4(x)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else: \n",
    "            target = targets[:, -1]\n",
    "            loss = F.cross_entropy(logits, target)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for i in range(max_new_tokens):\n",
    "            # get prediction from forward()\n",
    "            logits, _ = self(idx[:, i:])\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "        \n",
    "xb, yb = get_batch('train')\n",
    "m2 = MLP_M(vocab_size).to(device)\n",
    "logits, loss = m2(xb, yb)\n",
    "print(xb.shape)\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e20c663c-84e3-4043-8959-6d8eb3ca5431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "optimizer2 = torch.optim.AdamW(m2.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "672bd57f-8f49-4f4d-a480-862fe23d0a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/50000 : {'train': 4.164349555969238, 'val': 4.165612697601318}\n",
      "10000/50000 : {'train': 1.6873383522033691, 'val': 1.8061630725860596}\n",
      "20000/50000 : {'train': 1.6028873920440674, 'val': 1.7605390548706055}\n",
      "30000/50000 : {'train': 1.5525975227355957, 'val': 1.7518310546875}\n",
      "40000/50000 : {'train': 1.5078599452972412, 'val': 1.7562731504440308}\n"
     ]
    }
   ],
   "source": [
    "max_steps = 50000\n",
    "for steps in range(max_steps):\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    logits, loss = m2(xb, yb)\n",
    "    optimizer2.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer2.step()\n",
    "\n",
    "    if steps % 10000 == 0:\n",
    "        print(f\"{steps}/{max_steps} : {estimate_loss(m2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc69fa9e-83ba-4197-b96e-e6b3276f674b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        hagge\n",
      "If the noble run my man, thou us:\n",
      "A quickate a king, we so for unworn times in Sorrow him, this will so was some to take all with have a lead\n",
      "Which you are are such and out son and voice of requ\n"
     ]
    }
   ],
   "source": [
    "m2.eval(); # so BatchNorm layers will use the running mean and var\n",
    "idx = torch.ones((1, block_size), dtype=torch.long, device=device)\n",
    "print(decode(m2.generate(idx=idx, max_new_tokens=200)[0].tolist()))\n",
    "m2.train();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
