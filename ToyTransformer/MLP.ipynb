{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b762e50-ac98-4836-93d3-a4c063de0872",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "999fa2f5-d6d3-4404-9a73-8b545c63e17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# hardware acceleration\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c63434f-ffb3-4c7f-9686-b92bb5524704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already have input data\n"
     ]
    }
   ],
   "source": [
    "# get input\n",
    "if not os.path.exists('input.txt'):\n",
    "    import requests\n",
    "    data = requests.get('https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt')\n",
    "    with open('input.txt', 'w') as f:\n",
    "        f.write(data.text)\n",
    "    print('finished downloading input data')\n",
    "else:\n",
    "    print('already have input data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfabcd25-9956-4ed1-aeeb-bf816a27b9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_chars: 1115394\n"
     ]
    }
   ],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    print('n_chars:', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06d168e5-55fc-4ea9-a12e-0396cf170070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "caf02a28-a887-4d19-bf43-92583557022e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "vocab_size: 65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print('vocab_size:', vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f192b5b-a883-4f39-9b45-232517033974",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "We need to convert the raw text data as a sequence of integers according to some encoding or structure. Since this is a character level model, we just create a bijective mapping from characters to integers, but there are much more sophisticated tokenization techniques like BPE used by [OpenAi](https://github.com/openai/tiktoken), which is a sub-word tokenizer.\n",
    "\n",
    "Sub-word tokenizers are desirable because they allow the model to better understand grammer by breaking up words into chunks that are of common form. For example the tokenizer may encode '-ing' '-ed' particles into their own chunks so the model can learn the tense of a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f1f1f95-a676-4501-abce-330959382b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mapping from characters to integers\n",
    "stoi = {ch:i for i, ch in enumerate(chars)}\n",
    "itos = {i:ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s] # takes a string: outputs a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # takes a list of integers, output a string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "913ee88a-e6e4-4f6c-989e-ffa0df8b5cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode text\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "# create training and validation splits\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9961d7a-66cf-413e-8960-019778d0011a",
   "metadata": {},
   "source": [
    "### Sequential learning\n",
    "Token generation is an **auto-regressive** task. That is, we want to predict the next value in the sequence based on previous values. A predicted value is modeled as the linear combination of its previous outputs plus some noise. Clearly, words, sentences and natural language in general are auto regressive in nature.\n",
    "\n",
    "One of the benefits of training auto regressive models is that they are *self-supervised*. That is, we do not need to explicitly label our training data, as the corresponding output we want to predict is just the next value in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8d2683-9b08-4c0d-9660-351a81e0dced",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ecd7260-3058-4861-8967-769509567b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 8]) torch.Size([64, 8])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "block_size = 8 # also known as context_length\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]) # stack along dim 0\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "# why are we taking 8 outputs for each batch?\n",
    "xb, yb = get_batch('train')\n",
    "print(xb.shape, yb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b52692c-dea7-4890-9b54-b49dfd2c4abe",
   "metadata": {},
   "source": [
    "---\n",
    "### Review: Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef369c96-648a-4526-a28f-8cc252b5ab56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 65])\n",
      "tensor(4.7133, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "\n",
      "OMKQ\n",
      "'W:xQK-QzzbFz&TpM,zT$CZC'J-wqCccgbYeMa'B:.EUag! \n",
      "CMWukbsMer.?Xi ecUcMNm3LFXoUc$f:VDYA,$JS&iE&bZ\n"
     ]
    }
   ],
   "source": [
    "class BigramLM(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        logits = self.token_embedding_table(idx) # [batch_size, time/block_size, channels/vocab_size] -> [B, T, C]\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            # for F.cross_entropy, we need to flatten the B and T dims\n",
    "            # there are B batches and T characters in each batch (with C=vocab_size channels)\n",
    "            # so each row in the matrix below is some channel of a character from the batch\n",
    "            logits = logits.view(B*T, C) \n",
    "            # each entry in the vector below corresponds to the target value of each character in logits[B*T, C]\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get prediction from forward()\n",
    "            logits, loss = self(idx)\n",
    "            # only interested in predicting the next character\n",
    "            logits = logits[:, -1, :] # (B, C)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "m = BigramLM(vocab_size).to(device)\n",
    "logits, loss = m(xb, yb)\n",
    "\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "# 0 corresponds to new-line\n",
    "idx = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(idx=idx, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "215d8ce1-f000-4554-8685-22ab2eb0c031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train Bigram model\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "523edab8-fbb2-43fb-adf9-ee8060512185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate loss by taking an average loss over several batches\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model, eval_iters=100):\n",
    "    out = {}\n",
    "    model.eval() # set model to eval phase\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            _, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean().item()\n",
    "    model.train() # set model to train phase\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd01db93-bb90-4452-a59c-ce068e3fcc2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/10000 : {'train': 4.677175045013428, 'val': 4.660295486450195}\n",
      "1000/10000 : {'train': 3.6354596614837646, 'val': 3.6273064613342285}\n",
      "2000/10000 : {'train': 3.041649103164673, 'val': 3.029075860977173}\n",
      "3000/10000 : {'train': 2.7286901473999023, 'val': 2.7322945594787598}\n",
      "4000/10000 : {'train': 2.5985140800476074, 'val': 2.6089234352111816}\n",
      "5000/10000 : {'train': 2.5280611515045166, 'val': 2.5461597442626953}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m steps \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msteps\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_steps\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mestimate_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 11\u001b[0m, in \u001b[0;36mestimate_loss\u001b[0;34m(model, eval_iters)\u001b[0m\n\u001b[1;32m      9\u001b[0m         X, Y \u001b[38;5;241m=\u001b[39m get_batch(split)\n\u001b[1;32m     10\u001b[0m         _, loss \u001b[38;5;241m=\u001b[39m model(X, Y)\n\u001b[0;32m---> 11\u001b[0m         losses[k] \u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     out[split] \u001b[38;5;241m=\u001b[39m losses\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     13\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain() \u001b[38;5;66;03m# set model to train phase\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_steps = 10000\n",
    "for steps in range(max_steps):\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if steps % 1000 == 0:\n",
    "        print(f\"{steps}/{max_steps} : {estimate_loss(m)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659d2433-9ae3-48ad-95cb-2bc19c9442c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(idx=idx, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641b8bc3-d017-42f9-a47c-c4189d6fc873",
   "metadata": {},
   "source": [
    "---\n",
    "### Longer Context Length Model\n",
    "Pytorch implementation of what we had in MLP-3\n",
    "\n",
    "In addition, we will add a *positional embedding* layer along the *token embedding* layer. Hence, the model will also learn the positional relationship between different tokens in addition to their semantic  relationship learned by the token embedding layer. This layer will be crucial for self-attention in transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cf2930-34bf-4a12-b557-2fcd4c079c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_M(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        n_embd = 48\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.flatten = nn.Flatten(1)\n",
    "        self.layer1 = nn.Linear(block_size*n_embd, 100)\n",
    "        self.bn1 = nn.BatchNorm1d(100)\n",
    "        self.layer2 = nn.Linear(100, 200)\n",
    "        self.bn2 = nn.BatchNorm1d(200)\n",
    "        self.layer3 = nn.Linear(200, 300)\n",
    "        self.bn3 = nn.BatchNorm1d(300)\n",
    "        self.layer4 = nn.Linear(300, vocab_size)\n",
    "\n",
    "        self.relu1 = nn.LeakyReLU()\n",
    "        self.relu2 = nn.LeakyReLU()\n",
    "        self.relu3 = nn.LeakyReLU()\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        tok_emb = self.token_embedding_table(idx) \n",
    "        pos_emb = self.position_embedding_table(torch.arange(idx.shape[1], device=device))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.flatten(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu3(x)\n",
    "        logits = self.layer4(x)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else: \n",
    "            target = targets[:, -1]\n",
    "            loss = F.cross_entropy(logits, target)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for i in range(max_new_tokens):\n",
    "            # get prediction from forward()\n",
    "            logits, _ = self(idx[:, i:])\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "        \n",
    "xb, yb = get_batch('train')\n",
    "m2 = MLP_M(vocab_size).to(device)\n",
    "logits, loss = m2(xb, yb)\n",
    "print(xb.shape)\n",
    "print(logits.shape)\n",
    "print(sum(p.numel() for p in m.parameters()), 'parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20c663c-84e3-4043-8959-6d8eb3ca5431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "optimizer2 = torch.optim.AdamW(m2.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672bd57f-8f49-4f4d-a480-862fe23d0a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 50000\n",
    "for steps in range(max_steps):\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    logits, loss = m2(xb, yb)\n",
    "    optimizer2.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer2.step()\n",
    "\n",
    "    if steps % 10000 == 0:\n",
    "        print(f\"{steps}/{max_steps} : {estimate_loss(m2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc69fa9e-83ba-4197-b96e-e6b3276f674b",
   "metadata": {},
   "outputs": [],
   "source": [
    "m2.eval(); # so BatchNorm layers will use the running mean and var\n",
    "idx = torch.ones((1, block_size), dtype=torch.long, device=device)\n",
    "print(decode(m2.generate(idx=idx, max_new_tokens=200)[0].tolist()))\n",
    "m2.train();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
