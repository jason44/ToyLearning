{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4b762e50-ac98-4836-93d3-a4c063de0872",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "999fa2f5-d6d3-4404-9a73-8b545c63e17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# hardware acceleration\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cb40a27f-4014-4e4c-aad7-7ca644144e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "block_size = 128 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 200\n",
    "n_head = 6\n",
    "n_layer = 4\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3c63434f-ffb3-4c7f-9686-b92bb5524704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already have input data\n"
     ]
    }
   ],
   "source": [
    "# get input\n",
    "if not os.path.exists('input.txt'):\n",
    "    import requests\n",
    "    data = requests.get('https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt')\n",
    "    with open('input.txt', 'w') as f:\n",
    "        f.write(data.text)\n",
    "    print('finished downloading input data')\n",
    "else:\n",
    "    print('already have input data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dfabcd25-9956-4ed1-aeeb-bf816a27b9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_chars: 1115394\n"
     ]
    }
   ],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    print('n_chars:', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "caf02a28-a887-4d19-bf43-92583557022e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "vocab_size: 65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print('vocab_size:', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3f1f1f95-a676-4501-abce-330959382b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mapping from characters to integers\n",
    "stoi = {ch:i for i, ch in enumerate(chars)}\n",
    "itos = {i:ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s] # takes a string: outputs a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # takes a list of integers, output a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "913ee88a-e6e4-4f6c-989e-ffa0df8b5cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode text\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "# create training and validation splits\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2ecd7260-3058-4861-8967-769509567b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 128]) torch.Size([64, 128])\n"
     ]
    }
   ],
   "source": [
    "# loads data\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]) # stack along dim 0\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print(xb.shape, yb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6794eb23-f1b1-4852-810e-ba0024bcd5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate loss by taking an average loss over several batches\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model, eval_iters=100):\n",
    "    out = {}\n",
    "    model.eval() # set model to eval phase\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            _, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean().item()\n",
    "    model.train() # set model to train phase\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb48db5c-d953-4584-a2b0-c9eb35081a29",
   "metadata": {},
   "source": [
    "---\n",
    "### Precursor for Self-Attention\n",
    "In an *auto-regressive* model, we want to model each time step in terms of its previous time steps. However, we do not want each time step to be dependent on any future time step as our model is tasked with predicting future time steps. One way to model this relationship is by writing the channels of each time step as a linear combination, henceforth called *aggregation*, of the channels corresponding to the previous time steps. \n",
    "\n",
    "We can easily do this by multiplying each sample, which is $[T, C]$ matrix, by a *lower triangular* weighted matrix $\\mathrm{L}[T, T]$ whose rows sum to $1$. Initially, the weightings will be uniform, so the aggregation is really just the mean of previous channels. However, the weightings will later be learned by the model. The matrix multiplication $\\mathrm{L} \\times x$ has the same dimension as $x$, however each row of $x$ corresponding to the channels of each time step is now a weighted average of itself and the channels of the previous time steps.\n",
    "\n",
    "eg: $$\\begin{bmatrix}1.0 & 0.0 & 0.0 \\\\ 0.5 & 0.5 & 0.0 \\\\ 0.33 & 0.33 & 0.33\\end{bmatrix} \\times \\begin{bmatrix}a_1 & a_2 \\\\ b_1 & b_2\\\\ c_1 & c_2\\end{bmatrix}=\\begin{bmatrix}a_1 & a_2 \\\\ 0.5a_1 + 0.5b_1 & 0.5a_2 + 0.5b_2 \\\\ 0.33a_1 + 0.33b_1 + 0.33c_1 & 0.33a_2 + 0.33b_2 + 0.33c_2\\end{bmatrix}$$\n",
    "\n",
    "Although this does model each time step as a linear combination of its previous time steps, it does not retain any knowledge of the sequence, which makes it not ideal at its current state. In general, an Attention mechanism does not retain any spatial information, which is why a position embedding must be used for auto-regressive models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "13c642f6-5ab4-4014-b937-482f2491244f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# when we encode our vocabulary, our batch samples xb become a [B,T,C] tensor\n",
    "# where each sample is in a batch of size B containing.\n",
    "# each sample is a time varying sequence of length T (tokens)\n",
    "# and each time step contains a channel of information with length C (where C is dependent on the embedding)\n",
    "B, T, C = 4, 8, 2 # batch, time, channel\n",
    "\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "30f0efb6-b37c-4702-8b30-4a7aa4222c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "xbow = wei @ x # xbow short for x bag-of-words which refers to a model that disregards ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "38ca3aaa-cbb2-4de9-90d0-00af4bc82563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 2])\n"
     ]
    }
   ],
   "source": [
    "# Alternative calculation for x bag-of-words which will be useful for self-attention\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.ones((T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) # fills upper triangle with -inf\n",
    "wei = F.softmax(wei, dim=1) # exponentiate and normalize which will results in the same matrix as before\n",
    "xbow = wei @ x\n",
    "print(xbow.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61daee2c-5f22-4220-adc7-7eda362e1fd6",
   "metadata": {},
   "source": [
    "### Attention\n",
    "The main idea behind attention is that the weights $W$ of the expression relating the tokens $X$ with one another, $W \\times X$, should be learned by the model.\n",
    "**Attention** does this by doing the following.\n",
    "1. From each input token, whose count is denoted by `block_size`, `context_size`, or `T`, we create a *query* and *key* vector (we will use a simple linear layer, however there may be advantages to using more complex models to generate the key and query). The query vector, in abstract terms, describe a 'question' asked by that token. The key vector describes an 'answer'.\n",
    "2. We then get *affinities* between two different tokens by taking a dot product between their query and key vector.\n",
    "$$\\bf w = \\bf q \\times \\bf k^T$$\n",
    "Of course this operation is vectorized for all possible pairs by matrix multiplication.\n",
    "\n",
    "$$W = Q \\times K^T$$\n",
    "So $W$ represents the relationship between each token in the context. Larger values indicate two tokens whose query and key share some significant relationship.\n",
    "\n",
    "However, we do not want past tokens interacting with future tokens, so we will use the lower triangular technique established earlier on $W$. So for the *first* token in the context, its only relationship is with itself; this will always be the case.\n",
    "\n",
    "Then, we will softmax $W$ so that it becomes a *weighted* matrix, which we can aggregate with $X$. These values in $W$ are called **attention-scores** which tell us how much *attention* a token should be giving each token. \n",
    "\n",
    "However, we will not actually *attend* to the token embeddings directly. Rather, we will map the token embeddings to value vectors, which we will then attend to. There are several reasons to do this:\n",
    "1. We may want to map the embedding vector to a lower dimension. This is especially useful for **multi-head attention**, which runs several heads in parallel whose results we then concatenate. So if we use $6$ heads with no dimensionality reduction, then the output of such a layer will be `6*n_embd` which is too large. In the paper, $Q,K,V$ maps to a `n_embd//n_heads` space.\n",
    "2. A value *projection* allows each head to learn a perhpas more useful representation of each token for their own specific task.\n",
    "\n",
    "$$\\mathrm{Attention}(Q,K,V) = \\mathrm{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "where $d_k$ is the dimension of the key vectors. We additionally divide by $\\sqrt{d_k}$, because for large values of $d_k$ the variance of $QK^T$ grows very large, pushing the softmax function into regions where the gradient is extremely small. \n",
    "\n",
    "Attention is so effective because it does not favor any token more than the other, which is a problem of previous models for sequential data, like RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8810f7b6-20f9-4f79-a070-0e4523b9ae1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ee9534-5f67-48ac-9fa9-f681935a173f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "47148d90-b042-45fc-ac0b-e342199796b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 8, 16])\n",
      "torch.Size([32, 8, 16])\n"
     ]
    }
   ],
   "source": [
    "#B, T ,C = batch_size, block_size, vocab_size\n",
    "B,T,C = 32, 8, 16\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# A single Head perform self-attention \n",
    "head_size = C // 1\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x) # [B, T, hs]\n",
    "q = query(x) # [B, T, hs]\n",
    "wei = q @ k.transpose(-2, -1) # [B,T,hs] @ [B,hs,T] = [B,T,T]\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) \n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "v = value(x)\n",
    "out = wei @ v * (head_size**-0.5) # [B,T,T] @ [B,T,hs] = [B,T,hs]\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ecc53363-ceec-488e-bdce-d3f4f94abba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2858, 0.7142, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.8368, 0.1143, 0.0489, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0493, 0.3293, 0.3197, 0.3017, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1742, 0.3353, 0.0838, 0.2910, 0.1157, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0025, 0.0048, 0.9139, 0.0244, 0.0172, 0.0373, 0.0000, 0.0000],\n",
      "        [0.0492, 0.0300, 0.0917, 0.0390, 0.2185, 0.3188, 0.2528, 0.0000],\n",
      "        [0.5824, 0.0853, 0.0200, 0.0322, 0.0369, 0.1833, 0.0238, 0.0361]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(wei[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1423b06-8581-4271-b387-4ccf20ebccb2",
   "metadata": {},
   "source": [
    "---\n",
    "### Transformer Model\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d392b0e8-ca67-4c6a-8ba3-bd4e0e1187ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,hs)\n",
    "        q = self.query(x) # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,hs)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0e79df3-06e5-4e3f-93df-a2e01d6adf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd) # head_size * num_heads = n_embds SEE: Block\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1) # [B,T,n_embds]\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd3e3f9a-9910-4815-8811-b3ecc10fd2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e0375ec-86d2-4f77-91cb-38b6f05edb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ad72b7c-82f5-4510-b5cd-2e1915b0a743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1973665 parameters\n"
     ]
    }
   ],
   "source": [
    "class GPTLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = GPTLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters()), 'parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5ded6e9-7e8b-4480-b964-b7dd10d62149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.2057, val loss 4.2089\n",
      "step 500: train loss 1.9105, val loss 2.0045\n",
      "step 1000: train loss 1.5925, val loss 1.7777\n",
      "step 1500: train loss 1.4796, val loss 1.6743\n",
      "step 2000: train loss 1.4117, val loss 1.6086\n",
      "step 2500: train loss 1.3694, val loss 1.5855\n",
      "step 3000: train loss 1.3422, val loss 1.5666\n",
      "step 3500: train loss 1.3110, val loss 1.5522\n",
      "step 4000: train loss 1.2933, val loss 1.5334\n",
      "step 4500: train loss 1.2699, val loss 1.5194\n",
      "step 4999: train loss 1.2626, val loss 1.5138\n"
     ]
    }
   ],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss(m)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "17e0a13b-b465-4d26-bbb8-ede5c0cbd3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ive. Come, being to strange all the Lord.\n",
      "My foe worship, sweet fledly way my polach\n",
      "Master here baseness breath out where art what thou hast nothing.\n",
      "\n",
      "BENVOLIO:\n",
      "Lords, I do yet her; consul. Why, so I will lay a bawd\n",
      "tender of court? sir, the garlor penfrance,\n",
      "Which it is rankness to make upon my cousin\n",
      "Of this gracious belove to love were senself.\n",
      "\n",
      "Post:\n",
      "O fear\n",
      "Whom way over\n",
      "I meet you not another?\n",
      "\n",
      "First Servingman:\n",
      "No, within a peace fortune liqual\n",
      "With a dest one with that I'll any sake them\n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "m.eval()\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n",
    "m.train();\n",
    "#open('more.txt', 'w').write(decode(m.generate(context, max_new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6910578e-b305-4b4c-a228-3f2df3dee70d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
