{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b762e50-ac98-4836-93d3-a4c063de0872",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "999fa2f5-d6d3-4404-9a73-8b545c63e17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# hardware acceleration\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb40a27f-4014-4e4c-aad7-7ca644144e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "block_size = 128 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 204\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c63434f-ffb3-4c7f-9686-b92bb5524704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already have input data\n"
     ]
    }
   ],
   "source": [
    "# get input\n",
    "if not os.path.exists('input.txt'):\n",
    "    import requests\n",
    "    data = requests.get('https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt')\n",
    "    with open('input.txt', 'w') as f:\n",
    "        f.write(data.text)\n",
    "    print('finished downloading input data')\n",
    "else:\n",
    "    print('already have input data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfabcd25-9956-4ed1-aeeb-bf816a27b9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_chars: 1115394\n"
     ]
    }
   ],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    print('n_chars:', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "caf02a28-a887-4d19-bf43-92583557022e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "vocab_size: 65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print('vocab_size:', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f1f1f95-a676-4501-abce-330959382b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mapping from characters to integers\n",
    "stoi = {ch:i for i, ch in enumerate(chars)}\n",
    "itos = {i:ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s] # takes a string: outputs a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # takes a list of integers, output a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "913ee88a-e6e4-4f6c-989e-ffa0df8b5cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode text\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "# create training and validation splits\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ecd7260-3058-4861-8967-769509567b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 128]) torch.Size([64, 128])\n"
     ]
    }
   ],
   "source": [
    "# loads data\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]) # stack along dim 0\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print(xb.shape, yb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6794eb23-f1b1-4852-810e-ba0024bcd5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate loss by taking an average loss over several batches\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model, eval_iters=100):\n",
    "    out = {}\n",
    "    model.eval() # set model to eval phase\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            _, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean().item()\n",
    "    model.train() # set model to train phase\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb48db5c-d953-4584-a2b0-c9eb35081a29",
   "metadata": {},
   "source": [
    "---\n",
    "### Motivation for Self-Attention\n",
    "In an *auto-regressive* model, we want to model each time step in terms of its previous time steps. However, we do not want each time step to be dependent on any future time step as our model is tasked with predicting future time steps. One way to model this relationship is by writing the channels of each time step as a linear combination, henceforth called *aggregation*, of the channels corresponding to the previous time steps. \n",
    "\n",
    "We can easily do this by multiplying each sample, which is $[T, C]$ matrix, by a *lower triangular* weighted matrix $\\mathrm{L}[T, T]$ whose rows sum to $1$. Initially, the weightings will be uniform, so the aggregation is really just the mean of previous channels. However, the weightings will later be learned by the model. The matrix multiplication $\\mathrm{L} \\times x$ has the same dimension as $x$, however each row of $x$ corresponding to the channels of each time step is now a weighted average of itself and the channels of the previous time steps.\n",
    "\n",
    "eg: $$\\begin{bmatrix}1.0 & 0.0 & 0.0 \\\\ 0.5 & 0.5 & 0.0 \\\\ 0.33 & 0.33 & 0.33\\end{bmatrix} \\times \\begin{bmatrix}a_1 & a_2 \\\\ b_1 & b_2\\\\ c_1 & c_2\\end{bmatrix}=\\begin{bmatrix}a_1 & a_2 \\\\ 0.5a_1 + 0.5b_1 & 0.5a_2 + 0.5b_2 \\\\ 0.33a_1 + 0.33b_1 + 0.33c_1 & 0.33a_2 + 0.33b_2 + 0.33c_2\\end{bmatrix}$$\n",
    "\n",
    "Although this does model each time step as a linear combination of its previous time steps, it does not retain any knowledge of the sequence, which makes it not ideal at its current state. In general, an Attention mechanism does not retain any spatial information, which is why a position embedding must be used for auto-regressive models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13c642f6-5ab4-4014-b937-482f2491244f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# when we encode our vocabulary, our batch samples xb become a [B,T,C] tensor\n",
    "# where each sample is in a batch of size B containing.\n",
    "# each sample is a time varying sequence of length T (tokens)\n",
    "# and each time step contains a channel of information with length C (where C is dependent on the embedding)\n",
    "B, T, C = 4, 8, 2 # batch, time, channel\n",
    "\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30f0efb6-b37c-4702-8b30-4a7aa4222c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "xbow = wei @ x # xbow short for x bag-of-words which refers to a model that disregards ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38ca3aaa-cbb2-4de9-90d0-00af4bc82563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 2])\n"
     ]
    }
   ],
   "source": [
    "# Alternative calculation for x bag-of-words which will be useful for self-attention\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.ones((T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) # fills upper triangle with -inf\n",
    "wei = F.softmax(wei, dim=1) # exponentiate and normalize which will results in the same matrix as before\n",
    "xbow = wei @ x\n",
    "print(xbow.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "61daee2c-5f22-4220-adc7-7eda362e1fd6",
   "metadata": {},
   "source": [
    "## Attention\n",
    "The main idea behind attention is that the weights $W$ of the expression relating the tokens $X$ with one another, $W \\times X$, should be learned by the model.\n",
    "**Attention** does this by doing the following.\n",
    "1. From each input token, whose count is denoted by `block_size`, `context_size`, or `T`, we create a *query* and *key* vector (we will use a simple linear layer, however there may be advantages to using more complex models to generate the key and query). The query vector, in abstract terms, describe a 'question' asked by that token. The key vector describes an 'answer'.\n",
    "2. We then get *affinities* between two different tokens by taking a dot product between their query and key vector.\n",
    "$$\\bf w = \\bf q \\times \\bf k^T$$\n",
    "Of course this operation is vectorized for all possible pairs by matrix multiplication.\n",
    "\n",
    "$$W = Q \\times K^T$$\n",
    "So $W$ is a $T\\times T$ matrix representing the relationship between each token in the context. Larger values indicate two tokens whose query and key share some significant relationship.\n",
    "\n",
    "However, we do not want past tokens interacting with future tokens, so we will use the lower triangular technique established earlier on $W$. eg: for the *first* token in the context, its only relationship is with itself; this will always be the case.\n",
    "\n",
    "Then, we will softmax $W$ so that it becomes a *weighted* matrix, which we can aggregate with $X$. These values in $W$ are called **attention-scores** which tell us how much *attention* a token should be giving each token. \n",
    "\n",
    "However, we will not actually *attend* to the token embeddings directly. Rather, we will map the token embeddings to value vectors, which we will then attend to. There are several reasons to do this:\n",
    "1. We may want to map the embedding vector to a lower dimension. This is especially useful for **multi-head attention**, which runs several heads in parallel whose results we then concatenate. If we were to use $6$ heads with no dimensionality reduction, then the output of such a layer will be `6*n_embd` which is too large. In the paper, $Q,K,V$ maps to a `n_embd//n_heads` space, so the computational cost is very similar to a single `n_embd` dimensional head.\n",
    "2. A value *projection* allows each head to learn a perhaps more useful representation of each token for their own specific task.\n",
    "\n",
    "$$\\mathrm{Attention}(Q,K,V) = \\mathrm{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "where $d_k$ is the dimension of the key vectors. We additionally divide by $\\sqrt{d_k}$, because for large values of $d_k$ the variance of $QK^T$ grows very large, pushing the softmax function into regions where the gradient is extremely small. \n",
    "\n",
    "#### Multi-Head Attention\n",
    "Multi-head attention is beneficial because it allows each head to learn their own $Q,K,V$. So the information we are aggregating is more diverse than if we had used single head attention.\n",
    "\n",
    "#### Attention Block\n",
    "An attention block is a stack of layers starting with a multi-head attention layer followed by a multi-layer perceptron. \n",
    "First, each head in the multi-head layer performs self-attention, then the outputs are aggregated and learned by the MLP, which crucially provide non-linearity to the model. In abstract terms, the heads learn the relationship between tokens, and the MLP learns the meaning of such relationships with respect to the task.\n",
    "\n",
    "#### Benefits of Attention\n",
    "Attention is so effective because it does not favor any token more than the other, which is a problem of previous models for handling sequential data, like RNN. Each self-attention block also runs once sequentially, while a recursive layer runs $\\mathcal O(n)$ times, where $n$ is the length of the context, `block_size`.\n",
    "\n",
    "### Positional Encoding\n",
    "Unlike convolutional layers and recurrent networks, attention blocks by themselves do not contain any information about the relative or absolute positions of each token, so we will add a *positional encoding* to the model. Hence, the model will also learn the positional relationship between different tokens in addition to their semantic  relationship learned by the *token embedding* layer. This layer is crucial for the transformer model. The positional encodings can be either learned or fixed. The original paper notes that both give similar results, but we will use *learned* encodings here.\n",
    "\n",
    "### Encoder & Decoder Block\n",
    "The attention block defined above is called a *decoder block*, because it masks the the self-attention layer to prevent tokens from interacting with subsequent tokens. Attention blocks that allow all all tokens to interact regardless of position is called the *encoder block*. \n",
    "\n",
    "The decoder block is named as such because it is auto-regressive. Thus its purpose is to decode the structure of a sequence so the model can predict the future of the sequence. On the other hand, an encoder block encodes the structure of the whole sequence to something more meaningful to the model, which can be used in something like sentiment analysis.\n",
    "\n",
    "### Self-Attention & Cross-Attention\n",
    "Self-Attention is when the query, keys, and values all come from the same source. Cross-Attention is when the key and values come from a different source from the query. So cross attention relates the tokens in one source to the tokens in another source. \n",
    "\n",
    "In the original paper, cross attention was used to translate between two languages. For example, the encoder would encode a sequence of French, then its output would be passed as key and value for the decoder block whose purpose was to predict english tokens. Here, the decoder block has contextual information from the previous english tokens in the predicted sequence, but it also has information from the *entire* French sequence. Usually special tokens like `<START>` and `<END>` are used to denote the start of a decode sequence and its end.\n",
    "\n",
    "### Residual Connections\n",
    "As the number of layers in a NN increase, the NN will often come across the **vanishing gradient problem**, which is when the gradients get progressively smaller as they are backpropagated through the network. In effect, the layers closer to the input begin to update very slowly or not at all, which prevents the network from learning effectively. This can happen for several reasons, \n",
    "1. Activation functions like $\\tanh$ and sigmoid have derivatives that are very small in certain ranges (which is remedied by normalization layers or activation functions like ReLU).\n",
    "2. For deeper networks, if the gradient of a node is less than $1$, then the gradient of the connected nodes in the previous layer shrinks. As the network grows deeper, the gradients will shrink  exponentially towards.\n",
    "\n",
    "**Residual connections** use skip connections to allow gradients to flow more directly through the network. A skip connection is created by simply adding the ouput at one point of the network to the output of a deeper layer, provided that they both have the same dimensions. Recall that the backpropagation of addition is just the gradient of the current layer, so even if the gradient of $x$ in the *main connection* vanishes, the gradient being propogated through the *residual connection* (the connection made through the addition) is still meaningful. \n",
    "\n",
    "There are many methods to laying out *residual block*, but the one that works best in practice is to project the output of a block to the same dimension as $x$. Then we add $x$ to that projection, which establishes the residual connection. It may be beneficial to add a nonlinearity and or norm layer before or after the addition. In general, any stack of layers with a residual connection from the input to the output is called a **residual block**.\n",
    "\n",
    "### Regularization\n",
    "#### LayerNorm\n",
    "All activation functions are centered about the origin. We want our activations to remain stable so we want to have, at least initially, a mean at $0$ so the activation is not biased towards any particular region. Furthermore, we want unit variance, so the weights aren't taking on extreme values early on. \n",
    "\n",
    "Recall that BatchNorm normalizes (at initialization) the values along the batch dimension eg: if the mini-batch is $X=[B, C]$, then $BN(X)$ normalizes about the $0$ dimension. LayerNorm normalizes the values along the $1$ dimension. In a Transformer, this means that the distribution of each token is normalized at initalization.\n",
    "\n",
    "LayerNorm is preferrable over BatchNorm in a Transformer model.\n",
    "\n",
    "#### Dropout\n",
    "Dropout is a regularization technique that randomly sets a specified number of nodes in a layer to zero during each epoch. The masked weights do not learn for that epoch. However, the nodes that are still 'on' learn differently because of the mask. Abstractly, this forces the model to learn an ensemble of models which are then combined when no nodes are masked. This helps in preventing overfitting as it would take much much longer for the weights to learn the trining dataset under such conditions. \n",
    "\n",
    "#### Early Stopping\n",
    "Early stopping ends training early when the change in the validation loss is smaller than a certain $\\delta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d657ff55-f175-41f8-8fa0-aa9a2cb490a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47148d90-b042-45fc-ac0b-e342199796b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 8, 16])\n"
     ]
    }
   ],
   "source": [
    "#B, T ,C = batch_size, block_size, vocab_size\n",
    "B,T,C = 32, 8, 16\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# A single Head perform self-attention \n",
    "head_size = C // 1\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x) # [B, T, hs]\n",
    "q = query(x) # [B, T, hs]\n",
    "wei = q @ k.transpose(-2, -1) # [B,T,hs] @ [B,hs,T] = [B,T,T]\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) \n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "v = value(x)\n",
    "out = wei @ v * (head_size**-0.5) # [B,T,T] @ [B,T,hs] = [B,T,hs]\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ecc53363-ceec-488e-bdce-d3f4f94abba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.7927, 0.2073, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1009, 0.1969, 0.7022, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0476, 0.1376, 0.1814, 0.6334, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1358, 0.5664, 0.0762, 0.0603, 0.1612, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0207, 0.2973, 0.2778, 0.2459, 0.0387, 0.1195, 0.0000, 0.0000],\n",
      "        [0.1891, 0.1309, 0.0590, 0.1273, 0.3027, 0.0179, 0.1732, 0.0000],\n",
      "        [0.2296, 0.0428, 0.0288, 0.0121, 0.0369, 0.1511, 0.0177, 0.4810]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(wei[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1423b06-8581-4271-b387-4ccf20ebccb2",
   "metadata": {},
   "source": [
    "---\n",
    "### Transformer Model\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d392b0e8-ca67-4c6a-8ba3-bd4e0e1187ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,hs)\n",
    "        q = self.query(x) # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,hs)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0e79df3-06e5-4e3f-93df-a2e01d6adf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd) # head_size * num_heads = n_embds SEE: Block\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1) # [B,T,n_embds]\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd3e3f9a-9910-4815-8811-b3ecc10fd2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e0375ec-86d2-4f77-91cb-38b6f05edb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # add x for residual connection & apply LayerNorm before each stack\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ad72b7c-82f5-4510-b5cd-2e1915b0a743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3061697 parameters\n"
     ]
    }
   ],
   "source": [
    "class GPTLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = GPTLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters()), 'parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c5ded6e9-7e8b-4480-b964-b7dd10d62149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.1663, val loss 4.1663\n",
      "step 500: train loss 1.9024, val loss 1.9957\n",
      "step 1000: train loss 1.5639, val loss 1.7436\n",
      "step 1500: train loss 1.4437, val loss 1.6480\n",
      "step 2000: train loss 1.3807, val loss 1.5906\n",
      "step 2500: train loss 1.3294, val loss 1.5590\n",
      "step 3000: train loss 1.2984, val loss 1.5477\n",
      "step 3500: train loss 1.2709, val loss 1.5186\n",
      "step 4000: train loss 1.2490, val loss 1.5073\n",
      "step 4500: train loss 1.2231, val loss 1.5017\n",
      "step 4999: train loss 1.2046, val loss 1.4853\n"
     ]
    }
   ],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss(m)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "17e0a13b-b465-4d26-bbb8-ede5c0cbd3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HASTINGS:\n",
      "Fairly servant, I live a world,\n",
      "But Isabel, the brave mally her violent\n",
      "But, therefore at his waffords. Lay his Gaunt's learn:\n",
      "His own invade?\n",
      "\n",
      "JULIET:\n",
      "Nay, then all wilt our house.\n",
      "\n",
      "FLORIZEL:\n",
      "I'll we give a lineast:\n",
      "Go; you are going to what the earl of you:\n",
      "Discover me respected\n",
      "In safety. You have been not his innumation care.\n",
      "\n",
      "AUTOLYCUS:\n",
      "There's words, why deven thy hearts and this?\n",
      "\n",
      "CORIOLANUS:\n",
      "O, O Bianca, I am the countympanished;\n",
      "Yet his beaul against this give; so long much hi\n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "m.eval()\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n",
    "m.train();\n",
    "#open('more.txt', 'w').write(decode(m.generate(context, max_new_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c714e299-bf2a-43a8-b4b7-472c7e1ba925",
   "metadata": {},
   "source": [
    "### Pretraining\n",
    "In the pretraining stage, we train the decoder using some dataset. The result is some random word generator that just generates text its learned from the dataset (which is what we did).\n",
    "\n",
    "### Finetuning\n",
    "The finetuning stage follows from the pretraining stage and its purpose is to align the model to whatever task we want it to do, eg: rewrite a paragraph in a different tone, answer questions, interact with user, etc. This stage requires labeled data and several stages of supervised learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
