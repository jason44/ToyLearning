{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b762e50-ac98-4836-93d3-a4c063de0872",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "999fa2f5-d6d3-4404-9a73-8b545c63e17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# hardware acceleration\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c63434f-ffb3-4c7f-9686-b92bb5524704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already have input data\n"
     ]
    }
   ],
   "source": [
    "# get input\n",
    "if not os.path.exists('input.txt'):\n",
    "    import requests\n",
    "    data = requests.get('https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt')\n",
    "    with open('input.txt', 'w') as f:\n",
    "        f.write(data.text)\n",
    "    print('finished downloading input data')\n",
    "else:\n",
    "    print('already have input data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfabcd25-9956-4ed1-aeeb-bf816a27b9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_chars: 1115394\n"
     ]
    }
   ],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    print('n_chars:', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06d168e5-55fc-4ea9-a12e-0396cf170070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "caf02a28-a887-4d19-bf43-92583557022e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "vocab_size: 65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print('vocab_size:', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f1f1f95-a676-4501-abce-330959382b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mapping from characters to integers\n",
    "stoi = {ch:i for i, ch in enumerate(chars)}\n",
    "itos = {i:ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s] # takes a string: outputs a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # takes a list of integers, output a string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "913ee88a-e6e4-4f6c-989e-ffa0df8b5cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode text\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "# create training and validation splits\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ecd7260-3058-4861-8967-769509567b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 8]) torch.Size([64, 8])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "block_size = 8 # also known as context_length\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]) # stack along dim 0\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "# why are we taking 8 outputs for each batch?\n",
    "xb, yb = get_batch('train')\n",
    "print(xb.shape, yb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13c642f6-5ab4-4014-b937-482f2491244f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# when we encode our vocabulary, our batch samples xb become a [B,T,C] tensor\n",
    "# where each sample is in a batch of size B containing.\n",
    "# each sample is a time varying sequence of length T\n",
    "# and each time step contains a channel of information with length C (where C is dependent on the embedding)\n",
    "B, T, C = 4, 8, 2 # batch, time, channel\n",
    "\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb48db5c-d953-4584-a2b0-c9eb35081a29",
   "metadata": {},
   "source": [
    "### Precursor for Self-Attention\n",
    "In an *auto-regressive* model, we want to model each time step in terms of its previous time steps. However, we do not want each time step to be dependent on any future time step as our model is tasked with predicting future time stpes. One way to model this relationship is by writing the channels of each time step as a linear combination, henceforth called *aggregation*, of the channels corresponding to the previous time steps. \n",
    "\n",
    "We can easily do this by multiplying each sample, which is $[T, C]$ matrix, by a *lower triangular* weighted matrix $\\mathrm{L}[T, T]$ whose rows sum to $1$. Initially, the weightings will be uniform, so the aggregation is really just the mean of previous channels. However, the weightings will later be learned by the model. The matrix multiplication $\\mathrm{L} \\times x$ has the same dimension as $x$, however each row of $x$, corresponding to the channels of each time step is now a weighted average of itself and the channels of the previous time steps.\n",
    "\n",
    "eg: $$\\begin{bmatrix}1.0 & 0.0 & 0.0 \\\\ 0.5 & 0.5 & 0.0 \\\\ 0.33 & 0.33 & 0.33\\end{bmatrix} \\times \\begin{bmatrix}a_1 & a_2 \\\\ b_1 & b_2\\\\ c_1 & c_2\\end{bmatrix}=\\begin{bmatrix}a_1 & a_2 \\\\ 0.5a_1 + 0.5b_1 & 0.5a_2 + 0.5b_2 \\\\ 0.33a_1 + 0.33b_1 + 0.33c_1 & 0.33a_2 + 0.33b_2 + 0.33c_2\\end{bmatrix}$$\n",
    "\n",
    "Althoough this does model each time step as a linear combination of its previous time steps, it does not retain any knowledge of the sequence, which makes it not ideal at its current state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30f0efb6-b37c-4702-8b30-4a7aa4222c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "xbow = wei @ x # xbow short for x bag-of-words which refers to a model that disregards ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "38ca3aaa-cbb2-4de9-90d0-00af4bc82563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative calculation for x bag-of-words which will be useful for self-attention\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.ones((T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) # fills upper triangle with -inf\n",
    "wei = F.softmax(wei, dim=1) # exponentiate and normalize which will results in the same matrix as before\n",
    "xbow = wei @ x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1423b06-8581-4271-b387-4ccf20ebccb2",
   "metadata": {},
   "source": [
    "---\n",
    "### Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b452e4bb-3992-490d-aef0-eec5220ff7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        n_embd = 32\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, C)\n",
    "        x = tok_emb + pos_emb\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            # for F.cross_entropy, we need to flatten the B and T dims\n",
    "            # there are B batches and T characters in each batch (with C=vocab_size channels)\n",
    "            # so each row in the matrix below is some channel of a character from the batch\n",
    "            logits = logits.view(B*T, C) \n",
    "            # each entry in the vector below corresponds to the target value of each character in logits[B*T, C]\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get prediction from forward()\n",
    "            logits, loss = self(idx)\n",
    "            # only interested in predicting the next character\n",
    "            logits = logits[:, -1, :] # (B, C)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "m = BigramLM().to(device)\n",
    "logits, loss = m(xb, yb)\n",
    "\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "# 0 corresponds to new-line\n",
    "idx = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(idx=idx, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "38223440-ca4f-42bf-b456-836a5c1e8cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix C:\n",
      "[[ 58  64]\n",
      " [139 154]]\n",
      "\n",
      "Gradient of C with respect to A (B^T):\n",
      "[[ 7  9 11]\n",
      " [ 8 10 12]]\n",
      "\n",
      "Element-wise gradients with respect to A:\n",
      "[[[[ 7.  9. 11.]\n",
      "   [ 0.  0.  0.]]\n",
      "\n",
      "  [[ 8. 10. 12.]\n",
      "   [ 0.  0.  0.]]]\n",
      "\n",
      "\n",
      " [[[ 0.  0.  0.]\n",
      "   [ 7.  9. 11.]]\n",
      "\n",
      "  [[ 0.  0.  0.]\n",
      "   [ 8. 10. 12.]]]]\n",
      "\n",
      "Check if gradient_C_wrt_A matches the sum of element-wise gradients:\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define matrices A and B\n",
    "A = np.array([[1, 2, 3],\n",
    "              [4, 5, 6]])\n",
    "\n",
    "B = np.array([[7, 8],\n",
    "              [9, 10],\n",
    "              [11, 12]])\n",
    "\n",
    "# Compute matrix C\n",
    "C = np.dot(A, B)\n",
    "\n",
    "# Gradient of C with respect to A is B^T\n",
    "gradient_C_wrt_A = B.T\n",
    "\n",
    "# Display C and the gradient of C with respect to A\n",
    "print(\"Matrix C:\")\n",
    "print(C)\n",
    "\n",
    "print(\"\\nGradient of C with respect to A (B^T):\")\n",
    "print(gradient_C_wrt_A)\n",
    "\n",
    "# Compute element-wise gradients\n",
    "# Initialize a matrix to store element-wise gradients\n",
    "element_wise_gradients = np.zeros((C.shape[0], C.shape[1], A.shape[0], A.shape[1]))\n",
    "\n",
    "# Compute element-wise gradients\n",
    "for i in range(C.shape[0]):  # For each row in C\n",
    "    for j in range(C.shape[1]):  # For each column in C\n",
    "        for k in range(A.shape[0]):  # For each row in A\n",
    "            for l in range(A.shape[1]):  # For each column in A\n",
    "                if i == k:\n",
    "                    element_wise_gradients[i, j, k, l] = B[l, j]\n",
    "                else:\n",
    "                    element_wise_gradients[i, j, k, l] = 0\n",
    "\n",
    "print(\"\\nElement-wise gradients with respect to A:\")\n",
    "print(element_wise_gradients)\n",
    "\n",
    "# Verify that the gradient matrix B.T matches the element-wise gradients\n",
    "print(\"\\nCheck if gradient_C_wrt_A matches the sum of element-wise gradients:\")\n",
    "print(np.allclose(gradient_C_wrt_A, np.sum(element_wise_gradients, axis=(0, 1))))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
