{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48c72bf3-72c8-4acc-bea2-6105e8357490",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53387a0c-b43f-4a49-9c2f-bd6f543376af",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i, s, in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s, i in stoi.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4396ab9a-6b1d-4fc8-9526-65fcc239b7ae",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ab0c1d5-deeb-4328-a100-c7ccc1d5e3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emma\n",
      "... --> e\n",
      "..e --> m\n",
      ".em --> m\n",
      "emm --> a\n",
      "mma --> .\n",
      "olivia\n",
      "... --> o\n",
      "..o --> l\n",
      ".ol --> i\n",
      "oli --> v\n",
      "liv --> i\n",
      "ivi --> a\n",
      "via --> .\n",
      "ava\n",
      "... --> a\n",
      "..a --> v\n",
      ".av --> a\n",
      "ava --> .\n"
     ]
    }
   ],
   "source": [
    "# dataset\n",
    "\n",
    "block_size = 3 # context length: how many characters we take in order to predict the next one\n",
    "X, Y = [], []\n",
    "for w in words[:3]:\n",
    "    context = [0] * block_size\n",
    "    print(w)\n",
    "    for ch in w + '.':\n",
    "        ix = stoi[ch]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "        print(''.join(itos[i] for i in context), '-->', itos[ix])\n",
    "        context = context[1:] + [ix] # move the context window one char to the right\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7142fc64-cfd9-47b8-97da-df53c8b8c22b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 3]), torch.Size([16]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# each block of `block_size` is a sample input X[i]\n",
    "# and its corresponding Y[i] is the predicted label\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e4eddc-9010-4dca-9e2a-c8022ebd56a9",
   "metadata": {},
   "source": [
    "### Lookup table\n",
    "we want to embed the 27 characters of the english alphabet into $\\mathbb{R}^2$. We do this with a lookup table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5b4f17a-cb01-439f-8e69-70dbecd0ad49",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = torch.randn((27, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3aa8d17-3575-43ce-9dbb-b3520f137467",
   "metadata": {},
   "source": [
    "### Pytorch Indexing\n",
    "to do this, we can index into a pytorch tensor with another arrays/tensors and get something of equivalent dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88407636-9e48-4934-8e3d-c08b30d45efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.3316,  2.0169],\n",
      "        [ 0.5963,  1.0341],\n",
      "        [-0.4038, -0.6699]]) \n",
      "\n",
      "tensor([[ 0,  0,  0],\n",
      "        [ 0,  0,  5],\n",
      "        [ 0,  5, 13]]) \n",
      "\n",
      "tensor([[[-1.7021, -0.0617],\n",
      "         [-1.7021, -0.0617],\n",
      "         [-1.7021, -0.0617]],\n",
      "\n",
      "        [[-1.7021, -0.0617],\n",
      "         [-1.7021, -0.0617],\n",
      "         [-0.4512, -0.4796]],\n",
      "\n",
      "        [[-1.7021, -0.0617],\n",
      "         [-0.4512, -0.4796],\n",
      "         [ 1.6121, -0.6347]]])\n"
     ]
    }
   ],
   "source": [
    "print(C[[1, 2, 3]], '\\n') \n",
    "print(X[:3], '\\n')\n",
    "print(C[X][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8bbdf48-d17a-4388-ab40-5ec81331d1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3])\n",
      "torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "_t = torch.tensor((1, 2, 3))\n",
    "print(_t.shape)\n",
    "print(C[_t].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87c65211-9e15-4653-8be2-1f92b4e6aaa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "tensor(1) tensor([-1.3316,  2.0169]) tensor([-1.3316,  2.0169])\n"
     ]
    }
   ],
   "source": [
    "# similar to bigram NN, mat mul with one hot encoding just results in the row corresponding to the one hot encode\n",
    "F.one_hot(torch.tensor(3), num_classes=27).float() @ C, C[3]\n",
    "# you can think of C as the weights matrix of a NN layer\n",
    "\n",
    "# As shown above, we can map the domain (27 chars) to R^2 with C and pytorch indexing\n",
    "# furthermore, C is clearly invertible.\n",
    "print(itos[X[13, 2].item()])\n",
    "print(X[13, 2], C[X][13, 2], C[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59f0442b-c2f5-441c-b049-bab2b8d6ce58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3, 2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[X]\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78da9f6-c26d-4077-9fb3-de55d140fe7e",
   "metadata": {},
   "source": [
    "---\n",
    "### View\n",
    "Here, we try to manipulate the shape of our tensor `emb`, so that we can use it in the next layer which takes as input, a `[x, 6]` tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db48c5b6-65f1-4127-a392-a46bb13fff5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 2])\n",
      "torch.Size([16, 2])\n"
     ]
    }
   ],
   "source": [
    "M = emb[:, 0, :], emb[:, 1, :], emb[:, 2, :]\n",
    "print(M[0].shape)\n",
    "\n",
    "# torch.unbind removes the specified dimension from a given tensor, \n",
    "# and returns a tuple of slices along the removed dimension (obviously not including the removed dimension)\n",
    "#    basically, torch.unbind(n, 1) generalizes the result above\n",
    "# eg: if n.shape = [16, 3, 2], then torch.unbind(n, 1) is a tuple of 3 tensors of size [16, 2] \n",
    "# where the first entry in the tuple is n[:, 0, :] and so on.\n",
    "N = torch.unbind(emb, 1)\n",
    "print(N[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9365468e-de42-4a9c-96da-5e47883987a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.7021, -0.0617],\n",
      "        [-1.7021, -0.0617],\n",
      "        [-1.7021, -0.0617],\n",
      "        [-0.4512, -0.4796],\n",
      "        [ 1.6121, -0.6347],\n",
      "        [-1.7021, -0.0617],\n",
      "        [-1.7021, -0.0617],\n",
      "        [-1.7021, -0.0617],\n",
      "        [ 0.6503, -1.3409],\n",
      "        [ 0.8245,  0.1784],\n",
      "        [-1.3059,  0.0724],\n",
      "        [ 1.5494,  0.2942],\n",
      "        [-1.7021, -0.0617],\n",
      "        [-1.7021, -0.0617],\n",
      "        [-1.7021, -0.0617],\n",
      "        [-1.3316,  2.0169]])\n",
      "torch.Size([16, 2])\n"
     ]
    }
   ],
   "source": [
    "# In our case, we can see that torch.unbind(n,1)[0] corresponding to the embeded values of \n",
    "# the leftmost characters of the list of inputs X\n",
    "# likewise, torch.unbind(n,1)[1] are the embeded values of the second characters\n",
    "# and torch.unbind(n,1)[2], are the embeded values of the rightmost characters\n",
    "print(emb[:, 0, :])\n",
    "print(emb[:, 0, :].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc389d5a-e8f3-4b3b-aca7-4db64b70150e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.7021, -0.0617, -1.7021, -0.0617, -1.7021, -0.0617],\n",
       "         [-1.7021, -0.0617, -1.7021, -0.0617, -0.4512, -0.4796],\n",
       "         [-1.7021, -0.0617, -0.4512, -0.4796,  1.6121, -0.6347],\n",
       "         [-0.4512, -0.4796,  1.6121, -0.6347,  1.6121, -0.6347],\n",
       "         [ 1.6121, -0.6347,  1.6121, -0.6347, -1.3316,  2.0169],\n",
       "         [-1.7021, -0.0617, -1.7021, -0.0617, -1.7021, -0.0617],\n",
       "         [-1.7021, -0.0617, -1.7021, -0.0617,  0.6503, -1.3409],\n",
       "         [-1.7021, -0.0617,  0.6503, -1.3409,  0.8245,  0.1784],\n",
       "         [ 0.6503, -1.3409,  0.8245,  0.1784, -1.3059,  0.0724],\n",
       "         [ 0.8245,  0.1784, -1.3059,  0.0724,  1.5494,  0.2942],\n",
       "         [-1.3059,  0.0724,  1.5494,  0.2942, -1.3059,  0.0724],\n",
       "         [ 1.5494,  0.2942, -1.3059,  0.0724, -1.3316,  2.0169],\n",
       "         [-1.7021, -0.0617, -1.7021, -0.0617, -1.7021, -0.0617],\n",
       "         [-1.7021, -0.0617, -1.7021, -0.0617, -1.3316,  2.0169],\n",
       "         [-1.7021, -0.0617, -1.3316,  2.0169,  1.5494,  0.2942],\n",
       "         [-1.3316,  2.0169,  1.5494,  0.2942, -1.3316,  2.0169]]),\n",
       " torch.Size([16, 6]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concatinate the embeded values of each character along a specified dimension\n",
    "# eg: if n.shape, m.shape = [16, 2], then \n",
    "# torch.cat((n, m), 1).shape = [16, 4]\n",
    "# torch.cat((n, m), 0).shape = [32, 2]\n",
    "t = torch.cat(torch.unbind(emb, 1), 1)\n",
    "t, t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e424931-869c-4ce2-b74c-964a73f729f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17],\n",
       "        dtype=torch.uint8),\n",
       " torch.Size([18]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(18, dtype=torch.uint8)\n",
    "a, a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6928c29-3590-4476-80fd-b5bd5d96b45b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[0, 1, 2, 3, 4, 5, 6, 7, 8], [9, 10, 11, 12, 13, 14, 15, 16, 17]],\n",
       " [[[0, 1, 2], [3, 4, 5]],\n",
       "  [[6, 7, 8], [9, 10, 11]],\n",
       "  [[12, 13, 14], [15, 16, 17]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.view(2, 9).tolist(), a.view(3, 2, 3).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffc10d0-2812-4308-b040-8b94834074ab",
   "metadata": {},
   "source": [
    "`torch.tensor` has the `.view()` method which efficiently rearranges a tensor into a specified dimension. \n",
    "This can be done efficiently because the underlying data in a `torch.tensor` is stored in a one dimensional array. \n",
    "All `.view()` has to do is modify the strides and shapes.\n",
    "So, `.view()` is obviously preferrable to the method above, as it does all operations in-place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34575768-10d1-48ee-aeca-ecf4a1bcf72f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tensor.untyped_storage() returns a byte array (which is why the dtype is uint8)\n",
    "a.untyped_storage().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0140e12-69df-4185-81b6-feec63a2de9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.view([16, 6]) == torch.cat(torch.unbind(emb, 1), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611fd916-21ab-4ac4-bab6-8106e3933c47",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d85421f2-022a-486d-be95-418441117627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 100])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first hidden layer\n",
    "W1 = torch.randn(6, 100)\n",
    "b1 = torch.randn(100)\n",
    "# we can just use emb.view([-1, 6]) and pytorch will infer the shape\n",
    "h = torch.tanh(emb.view([emb.shape[0], 6]) @ W1 + b1)\n",
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4378c6ca-451c-4f41-99be-3e9558f59570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BROADCASTING\n",
    "# W1.shape = [32, 100]\n",
    "# b1.shape = [100]\n",
    "# --> 32, 100\n",
    "#      1, 100\n",
    "# so brodcasting rules dictate that b1 will be added to every row in W1, which is what we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b46ee1ef-ac4c-400d-ab59-92ab9b7e0ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1])\n",
      "torch.Size([16, 27])\n",
      "tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "# second (last) hidden layer\n",
    "W2 = torch.randn(100, 27)\n",
    "b2 = torch.randn(27)\n",
    "logits = h @ W2 + b2\n",
    "counts = logits.exp()\n",
    "# counts.sum(1) sums about the rows. \n",
    "# by broadcasting rules, each row (of size 27) in `counts` is then divided by the column vector counts.sum()\n",
    "# `prob`contains the probability of each character appearing for each input X[i]\n",
    "prob = counts / counts.sum(1, keepdims = True) \n",
    "print(counts.sum(1, keepdims = True).shape)\n",
    "print(prob.shape)\n",
    "print(prob[0].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ffd04c3-d3ff-497c-8922-70ca6a253b7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.5146e-09, 1.5124e-07, 2.7743e-11, 8.9618e-10, 6.5665e-03, 2.8617e-08,\n",
       "        1.6862e-12, 2.5899e-13, 6.8110e-05, 3.9547e-02, 1.2316e-10, 1.3935e-11,\n",
       "        6.2849e-05, 8.9503e-20, 3.3345e-04, 1.9316e-06])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# probabilities of the predicted labels\n",
    "# obviously the numbers aren't looking good as the NN is untrained.\n",
    "prob[torch.arange(prob.shape[0]), Y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d908e84-81ba-47b4-a5e9-b96946444b47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(18.4043)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# negative log-likelihood\n",
    "loss = -prob[torch.arange(prob.shape[0]), Y].log().mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c18e8338-4ccf-46a0-a82c-990b9d61ae31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(18.4043)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(logits, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537aa7c0-c7ac-49aa-8007-6120e10ef507",
   "metadata": {},
   "source": [
    "The calculation for the loss above is a very common calculation. It is just **classification**, if you think about it. \n",
    "Hence torch has a built in function `.cross_entropy()` for such computation that is much more efficient than our implementation.\n",
    "1. It computes the loss with much more efficient kernels that cluster multiple mathematical operations together thus reducing the runtime of the algorithm.\n",
    "2. The backwards pass of `F.cross_entropy()` is also more efficient than our calculations above because clustered mathematical operations often simplify into less computationally intensive operations (as in this case).\n",
    "3. `F.cross_entropy()` is more numerically stable in that it handles extreme values better than our approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "71f53cb1-c8c2-498b-862a-b9a2fd900a61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., nan])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# t_logits.exp() may overflow for large positive logits\n",
    "t_logits = torch.tensor([-5, -3, 0, 100])\n",
    "t_counts = t_logits.exp()\n",
    "t_probs = t_counts / t_counts.sum()\n",
    "t_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c1b4067f-0795-48e6-b836-3aa021728fe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Torch overcomes this by first noting that since probabilities are normalized,\n",
    "# we can offset the logits by any constant without changing the probability as demonstrated below:\n",
    "t_logits = torch.tensor([-5, -3, 0, 100]) - 100\n",
    "t_counts = t_logits.exp()\n",
    "t_probs = t_counts / t_counts.sum()\n",
    "\n",
    "t_logits2 = torch.tensor([-5, -3, 0, 100]) - 94\n",
    "t_counts2 = t_logits2.exp()\n",
    "t_probs2 = t_counts2 / t_counts2.sum()\n",
    "\n",
    "t_probs == t_probs2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd57b806-5a44-4c65-85a3-06790d0ae591",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <img src=\"mlp.png\" width=\"700\"/>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
