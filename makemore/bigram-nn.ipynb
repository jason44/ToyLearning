{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a24fc599-f1a8-4c54-ab04-073092425bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbae6370-8e72-4f26-94b6-1c97793e8770",
   "metadata": {},
   "source": [
    "### Neural Networks\n",
    "This is a visual representation of a neural network over a single sample.  \n",
    "<div>\n",
    "    <img src=\"nn_img.png\" width=\"150\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ab12e33-488e-41dc-9663-48af7d6e27ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26, '.': 0}\n"
     ]
    }
   ],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "#offset by 1 to account for our special character '.'\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "print(stoi)\n",
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed965a97-f5ef-43ce-9c1d-4b4b4dcb381e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of samples (bigrams):  228146\n",
      "torch.Size([228146])\n"
     ]
    }
   ],
   "source": [
    "# creating the bigram training set\n",
    "# xs <- input\n",
    "# ys <- labels (for use in the loss function and training)\n",
    "xs, ys = [], []\n",
    "\n",
    "for w in words[:]:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        xs.append(stoi[ch1])\n",
    "        ys.append(stoi[ch2])\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "print('number of samples (bigrams): ', len(xs))\n",
    "print(xs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5772ab3-0d37-417c-982d-5545732d76c2",
   "metadata": {},
   "source": [
    "##### One Hot Encoding\n",
    "One hot encoding encodes class indices (class labels) into vectors whose corresponding column bit is flipped.\n",
    "eg: 'a' will be a vector whose first entry is 1 and the rest 0.\n",
    "This is so the input can be fed into a NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b1b7e1a-825f-40da-8e6d-53a81dbfc588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([228146, 27])\n",
      "torch.Size([228146])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7609751c0d10>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhYAAACHCAYAAABK4hAcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAN2klEQVR4nO3df2hV9ePH8dfd2q4/urs6137cNufUUmpukrolkgkbTgvJ9A8r/1hDjOoqzlHJAl1CsDAIqSQjKP/xV0ImyQdDlpsE8wcTMaH21SFfr8xtKR/vdOZcu+/PH3263+9Nnd7tvXt2r88HHLj33Df3vHjzlr0899x7XMYYIwAAAAuSnA4AAAASB8UCAABYQ7EAAADWUCwAAIA1FAsAAGANxQIAAFhDsQAAANY8EsuDhUIhtbe3y+PxyOVyxfLQAABgkIwxun79unw+n5KSBj4nEdNi0d7erry8vFgeEgAAWBIIBJSbmzvgmJgWC4/HI0n631OTlPbo0D6FefnJGTYiAQCA+/hTffpZ/wr/HR9ITIvF3x9/pD2apDTP0IrFI64UG5EAAMD9/PfmHw9yGQMXbwIAAGsoFgAAwBqKBQAAsGZQxWLbtm2aNGmSRo0apdLSUp04ccJ2LgAAEIeiLhZ79+5VTU2N6urqdOrUKRUXF6uiokJdXV3DkQ8AAMSRqIvFJ598otWrV6uqqkpPPfWUtm/frjFjxujrr78ejnwAACCORFUsbt++rZaWFpWXl//fGyQlqby8XM3NzXeM7+3tVXd3d8QGAAASV1TF4sqVK+rv71dWVlbE/qysLHV0dNwxvr6+Xl6vN7zxq5sAACS2Yf1WSG1trYLBYHgLBALDeTgAAOCwqH55MyMjQ8nJyers7IzY39nZqezs7DvGu91uud3uoSUEAABxI6ozFqmpqZo1a5YaGhrC+0KhkBoaGjR37lzr4QAAQHyJ+l4hNTU1qqys1OzZs1VSUqKtW7eqp6dHVVVVw5EPAADEkaiLxYoVK/T7779r06ZN6ujo0MyZM3Xo0KE7LugEAAAPH5cxxsTqYN3d3fJ6vfr3/0we8t1NK3wz7YQCAAAD+tP0qVEHFAwGlZaWNuBY7hUCAACsifqjEBtefnKGHnGlOHHoh86P7aetvA9niAAAD4IzFgAAwBqKBQAAsIZiAQAArKFYAAAAaygWAADAGooFAACwhmIBAACsoVgAAABrKBYAAMAaigUAALCGYgEAAKyhWAAAAGsoFgAAwBqKBQAAsIZiAQAArKFYAAAAaygWAADAGooFAACw5hGnA2B4VfhmOh0BCeLH9tNW3oc1CSQ2zlgAAABrKBYAAMAaigUAALCGYgEAAKyJqljU19drzpw58ng8yszM1NKlS9Xa2jpc2QAAQJyJqlg0NTXJ7/fr2LFjOnz4sPr6+rRw4UL19PQMVz4AABBHovq66aFDhyKe79ixQ5mZmWppadH8+fOtBgMAAPFnSL9jEQwGJUnp6el3fb23t1e9vb3h593d3UM5HAAAGOEGffFmKBRSdXW15s2bp8LCwruOqa+vl9frDW95eXmDDgoAAEa+QRcLv9+vs2fPas+ePfccU1tbq2AwGN4CgcBgDwcAAOLAoD4KWbNmjQ4ePKijR48qNzf3nuPcbrfcbvegwwEAgPgSVbEwxmjt2rXav3+/GhsbVVBQMFy5AABAHIqqWPj9fu3atUsHDhyQx+NRR0eHJMnr9Wr06NHDEhAAAMSPqK6x+OKLLxQMBrVgwQLl5OSEt7179w5XPgAAEEei/igEAADgXrhXCAAAsIZiAQAArKFYAAAAaygWAADAGooFAACwhmIBAACsoVgAAABrKBYAAMAaigUAALCGYgEAAKyhWAAAAGsoFgAAwBqKBQAAsIZiAQAArKFYAAAAaygWAADAGooFAACwhmIBAACsoVgAAABrKBYAAMAaigUAALDmEacDDNaP7aetvVeFb6a19wISFf9OADwIzlgAAABrKBYAAMAaigUAALCGYgEAAKwZUrH46KOP5HK5VF1dbSkOAACIZ4MuFidPntSXX36poqIim3kAAEAcG1SxuHHjhlauXKmvvvpK48ePt50JAADEqUEVC7/frxdffFHl5eUDjuvt7VV3d3fEBgAAElfUP5C1Z88enTp1SidPnrzv2Pr6em3evHlQwQAAQPyJ6oxFIBDQunXrtHPnTo0aNeq+42traxUMBsNbIBAYdFAAADDyRXXGoqWlRV1dXXrmmWfC+/r7+3X06FF9/vnn6u3tVXJycvg1t9stt9ttLy0AABjRoioWZWVl+uWXXyL2VVVVafr06dqwYUNEqQAAAA+fqIqFx+NRYWFhxL6xY8dqwoQJd+wHAAAPH355EwAAWDPk26Y3NjZaiAEAABIBZywAAIA1Qz5jEQ1jjCTpT/VJZmjv1X09ZCHRX/40fdbeCwCARPOn/vo7+fff8YG4zIOMsuTSpUvKy8uL1eEAAIBFgUBAubm5A46JabEIhUJqb2+Xx+ORy+W657ju7m7l5eUpEAgoLS0tVvEeWsx37DDXscV8xxbzHVuxnG9jjK5fvy6fz6ekpIGvoojpRyFJSUn3bTr/X1paGoszhpjv2GGuY4v5ji3mO7ZiNd9er/eBxnHxJgAAsIZiAQAArBmRxcLtdquuro77jMQI8x07zHVsMd+xxXzH1kid75hevAkAABLbiDxjAQAA4hPFAgAAWEOxAAAA1lAsAACANRQLAABgzYgrFtu2bdOkSZM0atQolZaW6sSJE05HSkgffPCBXC5XxDZ9+nSnYyWMo0ePasmSJfL5fHK5XPr+++8jXjfGaNOmTcrJydHo0aNVXl6uc+fOORM2Adxvvl9//fU71vuiRYucCRvn6uvrNWfOHHk8HmVmZmrp0qVqbW2NGHPr1i35/X5NmDBBjz76qJYvX67Ozk6HEse3B5nvBQsW3LG+33zzTYcSj7BisXfvXtXU1Kiurk6nTp1ScXGxKioq1NXV5XS0hPT000/r8uXL4e3nn392OlLC6OnpUXFxsbZt23bX17ds2aJPP/1U27dv1/HjxzV27FhVVFTo1q1bMU6aGO4335K0aNGiiPW+e/fuGCZMHE1NTfL7/Tp27JgOHz6svr4+LVy4UD09PeEx69ev1w8//KB9+/apqalJ7e3tWrZsmYOp49eDzLckrV69OmJ9b9myxaHEkswIUlJSYvx+f/h5f3+/8fl8pr6+3sFUiamurs4UFxc7HeOhIMns378//DwUCpns7Gzz8ccfh/ddu3bNuN1us3v3bgcSJpZ/zrcxxlRWVpqXXnrJkTyJrqury0gyTU1Nxpi/1nJKSorZt29feMyvv/5qJJnm5manYiaMf863McY8//zzZt26dc6F+ocRc8bi9u3bamlpUXl5eXhfUlKSysvL1dzc7GCyxHXu3Dn5fD5NnjxZK1eu1MWLF52O9FC4cOGCOjo6Ita61+tVaWkpa30YNTY2KjMzU9OmTdNbb72lq1evOh0pIQSDQUlSenq6JKmlpUV9fX0R63v69OmaOHEi69uCf87333bu3KmMjAwVFhaqtrZWN2/edCKepBjf3XQgV65cUX9/v7KysiL2Z2Vl6bfffnMoVeIqLS3Vjh07NG3aNF2+fFmbN2/Wc889p7Nnz8rj8TgdL6F1dHRI0l3X+t+vwa5FixZp2bJlKigoUFtbm95//30tXrxYzc3NSk5Odjpe3AqFQqqurta8efNUWFgo6a/1nZqaqnHjxkWMZX0P3d3mW5Jee+015efny+fz6cyZM9qwYYNaW1v13XffOZJzxBQLxNbixYvDj4uKilRaWqr8/Hx9++23WrVqlYPJAPteeeWV8OMZM2aoqKhIU6ZMUWNjo8rKyhxMFt/8fr/Onj3L9Vkxcq/5fuONN8KPZ8yYoZycHJWVlamtrU1TpkyJdcyRc/FmRkaGkpOT77hyuLOzU9nZ2Q6leniMGzdOTz75pM6fP+90lIT393pmrTtn8uTJysjIYL0PwZo1a3Tw4EEdOXJEubm54f3Z2dm6ffu2rl27FjGe9T0095rvuyktLZUkx9b3iCkWqampmjVrlhoaGsL7QqGQGhoaNHfuXAeTPRxu3LihtrY25eTkOB0l4RUUFCg7OztirXd3d+v48eOs9Ri5dOmSrl69ynofBGOM1qxZo/379+unn35SQUFBxOuzZs1SSkpKxPpubW3VxYsXWd+DcL/5vpvTp09LkmPre0R9FFJTU6PKykrNnj1bJSUl2rp1q3p6elRVVeV0tITzzjvvaMmSJcrPz1d7e7vq6uqUnJysV1991eloCeHGjRsR/1u4cOGCTp8+rfT0dE2cOFHV1dX68MMP9cQTT6igoEAbN26Uz+fT0qVLnQsdxwaa7/T0dG3evFnLly9Xdna22tra9N5772nq1KmqqKhwMHV88vv92rVrlw4cOCCPxxO+bsLr9Wr06NHyer1atWqVampqlJ6errS0NK1du1Zz587Vs88+63D6+HO/+W5ra9OuXbv0wgsvaMKECTpz5ozWr1+v+fPnq6ioyJnQTn8t5Z8+++wzM3HiRJOammpKSkrMsWPHnI6UkFasWGFycnJMamqqefzxx82KFSvM+fPnnY6VMI4cOWIk3bFVVlYaY/76yunGjRtNVlaWcbvdpqyszLS2tjobOo4NNN83b940CxcuNI899phJSUkx+fn5ZvXq1aajo8Pp2HHpbvMsyXzzzTfhMX/88Yd5++23zfjx482YMWPMyy+/bC5fvuxc6Dh2v/m+ePGimT9/vklPTzdut9tMnTrVvPvuuyYYDDqW2fXf4AAAAEM2Yq6xAAAA8Y9iAQAArKFYAAAAaygWAADAGooFAACwhmIBAACsoVgAAABrKBYAAMAaigUAALCGYgEAAKyhWAAAAGv+A6sEjbDe9GoiAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xenc = F.one_hot(xs, num_classes=27).float()\n",
    "print(xenc.shape)\n",
    "print(xs.shape)\n",
    "plt.imshow(xenc[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a7bde1b-8055-46b3-88ee-a03d7830c2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly initialize the weights of 27 neurons. each neuron receives 27 inputs (xs), so they get 27 weights each.\n",
    "# the output of each neurons represent the log probability of a character appearing next\n",
    "    # the output of each neuron through is the entries of the mat mul (xenc @ W)\n",
    "# basically, W is a linear map T: float[27] -> float[27]\n",
    "W = torch.randn([27, 27], dtype=torch.float32, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9641bc-1136-4060-8da6-0d088c759428",
   "metadata": {},
   "source": [
    "---\n",
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f8a414a-6b9e-484f-87d1-de61c8a2fdd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.6987,  1.4856,  0.3359,  0.2004,  0.1515,  0.4962, -0.2049, -0.4369,\n",
      "        -0.0732, -0.2897,  0.3125,  0.9367,  0.2069,  0.9226, -0.6114, -0.3826,\n",
      "        -0.2972, -0.5829,  0.3995,  0.7398,  0.2564, -0.4321, -0.3426, -0.4539,\n",
      "        -0.6205, -0.0384, -0.0383])\n",
      "tensor([0.1829, 4.4178, 1.3992, 1.2219, 1.1636, 1.6425, 0.8147, 0.6460, 0.9294,\n",
      "        0.7485, 1.3669, 2.5516, 1.2298, 2.5159, 0.5426, 0.6821, 0.7429, 0.5583,\n",
      "        1.4911, 2.0956, 1.2923, 0.6492, 0.7099, 0.6351, 0.5377, 0.9623, 0.9624])\n",
      "tensor([0.0056, 0.1351, 0.0428, 0.0374, 0.0356, 0.0502, 0.0249, 0.0198, 0.0284,\n",
      "        0.0229, 0.0418, 0.0780, 0.0376, 0.0770, 0.0166, 0.0209, 0.0227, 0.0171,\n",
      "        0.0456, 0.0641, 0.0395, 0.0199, 0.0217, 0.0194, 0.0164, 0.0294, 0.0294])\n",
      "1.0000001192092896\n",
      "from model: 0.050242677330970764, actual: 1\n"
     ]
    }
   ],
   "source": [
    "# Example: for xs[0], the log-count of the next character is given by the vector\n",
    "logit = (xenc @ W)[0]\n",
    "counts = logit.exp()\n",
    "probs = counts / counts.sum()\n",
    "print(logit.data)\n",
    "print(counts.data)\n",
    "print(probs.data)\n",
    "print(probs.sum().item())\n",
    "# Since, this example is untrained, do not expect anything close to the actual value\n",
    "print(f\"from model: {probs[ys[0]]}, actual: 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bda7f9-e724-496b-8aa1-822ba4d4b1ab",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##### Forward Pass (Forward Propogation)\n",
    "the calculation and storage of intermediate variables (including outputs) for a neural network in order from the input layer to the output layer.\n",
    "\n",
    "We will interpret the outputs of the neurons as log counts.\n",
    "When a log count is exponentiated, we get a positive number we can interpret as the count.\n",
    "See the graph for $e^x$ for how this works.\n",
    "\n",
    "##### Softmax\n",
    "A softmax activation function is a common layer in a neural net that takes in a real vector and outputs another real vector. The output's entries are all positive and sum to 1. That is, the outputs are probabilities.\n",
    "This is very common in NNs because the output of neural networks often contain negative values and are not normalized. \n",
    "\n",
    "Note: matrix multiplication, exponentiation, summation, etc. are all differentiable operations and functions. This is important because we need to take the partial derivatives w.r.t to each weight during backpropogation\n",
    "\n",
    "##### Backward Pass (Back Propogation)\n",
    "Given the loss, we calculate the gradients with respect to each weight. Then, we perform gradient descent by adjusting each weight with the corresponding gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85efe533-2749-4ea7-bf8a-2040911da987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8435580730438232\n",
      "3.4316062927246094\n",
      "3.1786553859710693\n",
      "3.0190486907958984\n",
      "2.916682243347168\n",
      "torch.Size([27, 27])\n"
     ]
    }
   ],
   "source": [
    "# GRADIENT DESCENT\n",
    "for k in range(5):\n",
    "    # FORWARD PASS\n",
    "    logits = (xenc @ W) # log-counts\n",
    "    counts = logits.exp() # counts, (larger count = more likely) \n",
    "    probs = counts / counts.sum(1, keepdims=True) # probability for the next character\n",
    "    # Note: the last 2 lines above are together called a 'softmax activation function'\n",
    "    \n",
    "    # Normalized negative log likelihood\n",
    "        # we take the log likelihood of each predicted label, then we sum and normalize with .mean() to get a real-valued loss'\n",
    "        # Recall the graph of -ln(). Minimizing -ln() is akin to maximizing the probabilities of the predicted labels \n",
    "        # which is exactly what we want to do, as the predicted labels should have higher probability than not. \n",
    "    loss = -probs[torch.arange(len(xs)), ys].log().mean()    \n",
    "    # BACKWARD PASS\n",
    "    W.grad = None # set to zero the gradient\n",
    "    # pytorch keeps track of all operations on the tensor, \n",
    "    # so we can just call backward() to get the gradients with respect to each weight\n",
    "    loss.backward()\n",
    "\n",
    "    # UPDATES\n",
    "    eta = 50\n",
    "    # -eta to perform gradient descent as W.grad points in the direction of greatest increase\n",
    "    W.data += -eta * W.grad\n",
    "    print(loss.item())\n",
    "print(W.grad.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e727a494-1db7-4cc8-98bb-b8f259d838b7",
   "metadata": {},
   "source": [
    "---\n",
    "##### Log Likelihood \n",
    "The **likelihood** of an outcome is the product of the probabilities of each token eg: $a*b*c$.\n",
    "Since log is a monotonic function, we can use log likelihood in place of likelihood and still get the desired behavior.\n",
    "Moreover, the property that $\\log(a*b*c)=\\log(a)+\\log(b)+\\log(c)$ is convenient in vectorized calculations of log likelihood.\n",
    "\n",
    "Note: for performing gradient descent, we will be using the negative log likelihood function.\n",
    "The $\\log$ function has the property that it is 0 at `prob=1` and approaches $-\\infty$ as `prob` approaches 0.\n",
    "Clearly the $log$ function has no local minima in $[0, 1]$, but the negative log likelihood does.\n",
    "\n",
    "If we want to perform gradient ascent, we would use a gain function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ee3b6a6-32aa-4c97-b62d-56f4fba0eda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.053098954260349274, 0.017353732138872147, 0.030328070744872093, 0.20883412659168243, 0.1867654025554657]\n",
      "[-2.935598134994507, -4.053947448730469, -3.4956815242767334, -1.5662150382995605, -1.6779019832611084]\n",
      "2.7458691596984863\n"
     ]
    }
   ],
   "source": [
    "print(probs[torch.arange(len(xs)), ys][:5].tolist())\n",
    "print(probs[torch.arange(len(xs)), ys][:5].log().tolist())\n",
    "print(-probs[torch.arange(len(xs)), ys][:5].log().mean().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec1317f-f8cf-4fec-a3bf-8e33797b61ad",
   "metadata": {},
   "source": [
    "---\n",
    "##### Regularization\n",
    "Regularization is the process of smoothing the distribution of weights.\n",
    "This is done by adding a *regularization loss* term to the loss function \n",
    "whose purpose it is to penalizes nonzero weights.\n",
    "\n",
    "This is beneficial for reducing complexity. The more 0 weights we have, the less weights we have to process. \n",
    "\n",
    "The regularization below is an example of **L2 regularization**, with the L2 term $\\lambda \\mathbf{w}^2$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1ca19ef-896b-4790-8340-6a7bec48b2a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.9250, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_loss = -probs[torch.arange(len(xs)), ys].log().mean() + 0.01*(W**2).mean()\n",
    "reg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e88aa8c5-732f-4581-bece-8dd0443a1fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jf.\n",
      "c.\n",
      "mai.\n",
      "ayoa.\n",
      "e.\n"
     ]
    }
   ],
   "source": [
    "# SAMPLING FROM MODEL\n",
    "for i in range(5):\n",
    "    out = []\n",
    "    ix = 0;\n",
    "    \n",
    "    while True:\n",
    "        # begin with '.'\n",
    "        _xenc = F.one_hot(torch.tensor([ix]), num_classes=27).float()\n",
    "        _logits = _xenc @ W \n",
    "        _counts = _logits.exp()\n",
    "        P = _counts / _counts.sum(1, keepdims=True)\n",
    "        \n",
    "        ix = torch.multinomial(P, num_samples=1, replacement=True).item()\n",
    "        out.append(itos[ix])\n",
    "        if ix == 0:\n",
    "            break\n",
    "    print(''.join(out))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
