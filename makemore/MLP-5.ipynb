{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## makemore: part 5\n",
    "Culmulative review and new stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32033\n",
      "15\n",
      "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n"
     ]
    }
   ],
   "source": [
    "# read in all the words\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "print(len(words))\n",
    "print(max(len(w) for w in words))\n",
    "print(words[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[1;32m      3\u001b[0m random\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m random\u001b[38;5;241m.\u001b[39mshuffle(\u001b[43mwords\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'words' is not defined"
     ]
    }
   ],
   "source": [
    "# shuffle up the words (because our training data may have some sort of ordering)\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 8]) torch.Size([182625])\n",
      "torch.Size([22655, 8]) torch.Size([22655])\n",
      "torch.Size([22866, 8]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "block_size = 8 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):  \n",
    "  X, Y = [], []\n",
    "  \n",
    "  for w in words:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      context = context[1:] + [ix] # crop and append\n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "  return X, Y\n",
    "\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
    "Xte,  Yte  = build_dataset(words[n2:])     # 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........ --> y\n",
      ".......y --> u\n",
      "......yu --> h\n",
      ".....yuh --> e\n",
      "....yuhe --> n\n",
      "...yuhen --> g\n",
      "..yuheng --> .\n",
      "........ --> d\n",
      ".......d --> i\n",
      "......di --> o\n",
      ".....dio --> n\n",
      "....dion --> d\n",
      "...diond --> r\n",
      "..diondr --> e\n",
      ".diondre --> .\n",
      "........ --> x\n",
      ".......x --> a\n",
      "......xa --> v\n",
      ".....xav --> i\n",
      "....xavi --> e\n"
     ]
    }
   ],
   "source": [
    "for x,y in zip(Xtr[:20], Ytr[:20]):\n",
    "  print(''.join(itos[ix.item()] for ix in x), '-->', itos[y.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Near copy paste of the layers we have developed in Part 3\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class Linear:\n",
    "  \n",
    "  def __init__(self, fan_in, fan_out, bias=True):\n",
    "    self.weight = torch.randn((fan_in, fan_out)) / fan_in**0.5 # note: kaiming init\n",
    "    self.bias = torch.zeros(fan_out) if bias else None\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    self.out = x @ self.weight\n",
    "    if self.bias is not None:\n",
    "      self.out += self.bias\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [self.weight] + ([] if self.bias is None else [self.bias])\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class BatchNorm1d:\n",
    "  \n",
    "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "    self.eps = eps\n",
    "    self.momentum = momentum\n",
    "    self.training = True # determines whether we use the batch mean and var or running mean and var\n",
    "    # parameters (trained with backprop)\n",
    "    self.gamma = torch.ones(dim)\n",
    "    self.beta = torch.zeros(dim)\n",
    "    # buffers (trained with a running 'momentum update')\n",
    "    self.running_mean = torch.zeros(dim)\n",
    "    self.running_var = torch.ones(dim)\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    # calculate the forward pass\n",
    "    if self.training:\n",
    "      if x.ndim == 2:\n",
    "        dim = 0\n",
    "      elif x.ndim == 3:\n",
    "        dim = (0,1)\n",
    "      xmean = x.mean(dim, keepdim=True) # batch mean\n",
    "      xvar = x.var(dim, keepdim=True) # batch variance\n",
    "    else:\n",
    "      xmean = self.running_mean\n",
    "      xvar = self.running_var\n",
    "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "    self.out = self.gamma * xhat + self.beta\n",
    "    # update the buffers\n",
    "    if self.training:\n",
    "      with torch.no_grad():\n",
    "        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n",
    "        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [self.gamma, self.beta]\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class Tanh:\n",
    "  def __call__(self, x):\n",
    "    self.out = torch.tanh(x)\n",
    "    return self.out\n",
    "  def parameters(self):\n",
    "    return []\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class Embedding:\n",
    "  \n",
    "  def __init__(self, num_embeddings, embedding_dim):\n",
    "    self.weight = torch.randn((num_embeddings, embedding_dim))\n",
    "    \n",
    "  def __call__(self, IX):\n",
    "    self.out = self.weight[IX]\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [self.weight]\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class FlattenConsecutive:\n",
    "  \n",
    "  def __init__(self, n):\n",
    "    self.n = n\n",
    "\n",
    "  def __call__(self, x):\n",
    "    B, T, C = x.shape\n",
    "    # this view \n",
    "    x = x.view(B, T//self.n, C*self.n)\n",
    "    if x.shape[1] == 1:\n",
    "      x = x.squeeze(1)\n",
    "    self.out = x\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return []\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class Sequential:\n",
    "  \n",
    "  def __init__(self, layers):\n",
    "    self.layers = layers\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    for layer in self.layers:\n",
    "      x = layer(x)\n",
    "    self.out = x\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    # get parameters of all layers and stretch them out into one list\n",
    "    return [p for layer in self.layers for p in layer.parameters()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42); # seed rng for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Review: Neural Networks\n",
    "Note that it is redundant to stack two linear layers directly as the combination of two consecutive linear layers is mathematically equivalent to a single linear transformation:\n",
    "$$y = W_2 (W_1 X + b_1) + b_2=W_2W_1 X + (W_2b_1 + b_2)$$\n",
    "Which is why deep neural networks have the form shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75350\n"
     ]
    }
   ],
   "source": [
    "# original network\n",
    "n_embd = 24 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 145 # the number of neurons in the hidden layer of the MLP\n",
    "model = Sequential([\n",
    "   Embedding(vocab_size, n_embd),\n",
    "   FlattenConsecutive(8), Linear(n_embd * 8, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "   Linear(n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "   Linear(n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "   Linear(n_hidden, vocab_size),\n",
    "])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# hierarchical network\n",
    "n_embd = 24 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 128 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "model = Sequential([\n",
    "  Embedding(vocab_size, n_embd),\n",
    "  FlattenConsecutive(2), Linear(n_embd * 2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  FlattenConsecutive(2), Linear(n_hidden*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  FlattenConsecutive(2), Linear(n_hidden*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  Linear(n_hidden, vocab_size),\n",
    "])\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# parameter init\n",
    "with torch.no_grad():\n",
    "  model.layers[-1].weight *= 0.1 # last layer make less confident\n",
    "\n",
    "parameters = model.parameters()\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 3.2987\n",
      "  10000/ 200000: 2.3708\n",
      "  20000/ 200000: 1.8953\n",
      "  30000/ 200000: 1.8794\n",
      "  40000/ 200000: 2.1956\n",
      "  50000/ 200000: 2.0652\n",
      "  60000/ 200000: 1.5985\n",
      "  70000/ 200000: 2.3081\n",
      "  80000/ 200000: 2.0714\n",
      "  90000/ 200000: 1.7737\n",
      " 100000/ 200000: 2.0886\n",
      " 110000/ 200000: 1.7085\n",
      " 120000/ 200000: 1.7414\n",
      " 130000/ 200000: 2.3212\n",
      " 140000/ 200000: 1.8260\n",
      " 150000/ 200000: 1.6633\n",
      " 160000/ 200000: 1.6061\n",
      " 170000/ 200000: 1.6071\n",
      " 180000/ 200000: 2.3285\n",
      " 190000/ 200000: 1.7356\n"
     ]
    }
   ],
   "source": [
    "# same optimization as last time\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "  \n",
    "  # minibatch construct\n",
    "  ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
    "  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "  \n",
    "  # forward pass\n",
    "  logits = model(Xb)\n",
    "  loss = F.cross_entropy(logits, Yb) # loss function\n",
    "  \n",
    "  # backward pass\n",
    "  for p in parameters:\n",
    "    p.grad = None\n",
    "  loss.backward()\n",
    "  \n",
    "  # update: simple SGD\n",
    "  lr = 0.1 if i < 150000 else 0.01 # step learning rate decay\n",
    "  for p in parameters:\n",
    "    p.data += -lr * p.grad\n",
    "\n",
    "  # track stats\n",
    "  if i % 10000 == 0: # print every once in a while\n",
    "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "  lossi.append(loss.log10().item())\n",
    "\n",
    "  #if i == 20000 - 1: break # DEBUG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2, 3, 4],\n",
      "        [5, 6, 7, 8, 9]])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(torch.arange(0, 10).view(-1, 5)) # eg: with torch.view, rows are always consecutive\n",
    "\n",
    "#plt.plot(lossi) there is too much entropy during training for this graph to be actually useful\n",
    "# so we can take take the average of 1000 consecutive samples to get a much nicer graph\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.plot(torch.tensor(lossi).view(-1, 1000).mean(1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Review: Embedding Layer\n",
    "In our preprocessing step, we had each example represented as a `block_size` feature vector. Before we can feed it into the NN, we would need to one hot encode each entry in the block to get an `[block_size, 27]` matrix representation of the data, then flatten it into a vector. \n",
    "The purpose of the embedding layer is to allow us to use a smaller representation of the data.\n",
    "In short, `C[27, n_embd]` acts as a lookup table that maps the $27$ characters to a *smaller* `n_embd` vector space (whose values are learned during training). Hence, the tensor `emb` has a dimension of `[batch_size, block_size, n_embd]`, where each `[block_size, n_embd]` matrix corresponds to a sample. Then, we can flatten dim $1$ and dim $2$ together to get a vector representation of each sample for the next layer in the NN.\n",
    "\n",
    "This layer is not really necessary for (english) character level NNs as english does not use much characters, but this step because essential in *reducing dimensionality* for word level NNs which may have vocabularies in the tens of thousands (or chinese character level NNs which have thousands of characters).\n",
    "\n",
    "In addition, an embed layer lets the model learn how it should represent characters (or words) in a vocab instead of having the representation fixed (like in the case of one hot encoding). Which allows the model to capture *semantic relationships*. For example, vowels are semantically similar to one another, so they may be mapped more closely together in the `n_embd` vector space. This pattern is more noticable in word level models.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C: torch.Size([27, 24])\n",
      "Xb: torch.Size([32, 8])\n",
      "emb: torch.Size([32, 8, 24])\n",
      "embcat: torch.Size([32, 192])\n"
     ]
    }
   ],
   "source": [
    "_C = torch.randn(vocab_size, n_embd)\n",
    "print('C:', _C.shape)\n",
    "print('Xb:', Xb.shape)\n",
    "_emb = _C[Xb]\n",
    "print('emb:', _emb.shape)\n",
    "_embcat = _emb.view(_emb.shape[0], -1)\n",
    "print('embcat:', _embcat.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a : a | dist: tensor(0.)\n",
      "a : e | dist: tensor(20.8980)\n",
      "a : i | dist: tensor(25.0571)\n",
      "a : r | dist: tensor(27.4429)\n",
      "a : n | dist: tensor(28.6866)\n",
      "\n",
      "e : e | dist: tensor(0.)\n",
      "e : a | dist: tensor(20.8980)\n",
      "e : i | dist: tensor(22.7752)\n",
      "e : r | dist: tensor(29.6931)\n",
      "e : n | dist: tensor(32.9591)\n",
      "\n",
      "i : i | dist: tensor(0.)\n",
      "i : e | dist: tensor(22.7752)\n",
      "i : a | dist: tensor(25.0571)\n",
      "i : y | dist: tensor(29.9073)\n",
      "i : r | dist: tensor(31.9663)\n",
      "\n",
      "o : o | dist: tensor(0.)\n",
      "o : a | dist: tensor(30.7473)\n",
      "o : e | dist: tensor(37.7703)\n",
      "o : i | dist: tensor(38.1224)\n",
      "o : l | dist: tensor(44.7039)\n",
      "\n",
      "u : u | dist: tensor(0.)\n",
      "u : a | dist: tensor(47.8242)\n",
      "u : e | dist: tensor(53.8457)\n",
      "u : x | dist: tensor(56.7666)\n",
      "u : i | dist: tensor(56.9930)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    C = model.layers[0].weight\n",
    "\n",
    "@torch.no_grad()\n",
    "def k_nearest_neighbors(i, k):\n",
    "    neighbors = []\n",
    "    for j in range(1, vocab_size):\n",
    "        dist = torch.sum((C[i] - C[j])**2)\n",
    "        neighbors.append((dist, j))\n",
    "    neighbors = sorted(neighbors)\n",
    "    for n in neighbors[:k]:\n",
    "        j = n[1]\n",
    "        dist = n[0]\n",
    "        print(f\"{itos[i]} : {itos[j]} | dist: {str(dist):2}\")\n",
    "\n",
    "vowels = ['a', 'e', 'i', 'o', 'u']    \n",
    "for i in range(1, vocab_size):\n",
    "    if itos[i] in vowels:\n",
    "        k_nearest_neighbors(i, 5)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the 5 nearest neighbors of each vowel in the alphabet are mostly other vowels, which is what we'd expect considering their semantical similarities in the english language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Wavenet\n",
    "Increasing the `block_size` improves the performance of the model. However in our previous examples we have not been fully utilizing the additional information that comes from increasing the `block_size`, because we squashed the entire `[block_size, n_embd]` matrix into a single vector to pass into the next layer. Hence, every layer after that layer has no 'knowledge' about the sequential relationships between each character of a block.\n",
    "\n",
    "We want the model to have knowledge of the sequential relationships between characters in a block. WaveNet does this by flattening the example in each layer. It does so by combining consecutive pair of characters to make what is essentially a block of bigrams. Then it does the same thing for the next layer and so on until we have a vector representation of the example. **I'm not sure if this is true**\n",
    "\n",
    "##### Block in each layer of the WaveNet-like model\n",
    "`1 2 3 4 5 6 7 8` $\\longrightarrow$ `(1 2) (3 4) (5 6) (7 8)` $\\longrightarrow$ `(1 2 3 4) (5 6 7 8)` $\\longrightarrow$ `(1 2 3 4 5 6 7 8)`.\n",
    "\n",
    "In our earlier model, we flattened the sample after the embed layer, so each layer afterwards learns the weights of `(1 2 3 4 5 6 7 8)`, which does not have any sequential information. \n",
    "If instead the flattening layer was in the end of the NN, then each layer learns the weights of `1 2 3 4 5 6 7 8 9` which is not too different from a bigram.  **I'm not sure if this is true either**\n",
    "\n",
    "In a WaveNet-like model, each layer learns the weights of blocks (which double in length in each layer). So, the first layer learns the weights of bigrams. The next learns the weights of 'quadgrams' and so on, which obviously encodes sequence into the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 8, 24])\n",
      "torch.Size([32, 8, 145])\n",
      "torch.Size([32, 4, 290])\n"
     ]
    }
   ],
   "source": [
    "print(_emb.shape) # [batch_size, block_size, n_embd]\n",
    "\n",
    "# matrix multiplication for tensors with a rank greater than two just computes the batches in parallel \n",
    "# so the matmul below creates [batch_size, block_size] vectors of size [n_hidden]\n",
    "_x1 = _emb @ torch.rand((n_embd, n_hidden))\n",
    "print(_x1.shape) # [batch_size, block_size, n_hidden]\n",
    "_x1flat = _x1.view(batch_size, block_size//2, -1)\n",
    "print(_x1flat.shape) # [batch_size, block_size/2, n_hidden*2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Sample from Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put layers into eval mode (needed for batchnorm especially)\n",
    "for layer in model.layers:\n",
    "  layer.training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 1.7433949708938599\n",
      "val 2.0092387199401855\n"
     ]
    }
   ],
   "source": [
    "# evaluate the loss\n",
    "@torch.no_grad() # this decorator disables gradient tracking inside pytorch\n",
    "def split_loss(split):\n",
    "  x,y = {\n",
    "    'train': (Xtr, Ytr),\n",
    "    'val': (Xdev, Ydev),\n",
    "    'test': (Xte, Yte),\n",
    "  }[split]\n",
    "  logits = model(x)\n",
    "  loss = F.cross_entropy(logits, y)\n",
    "  print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### performance log\n",
    "\n",
    "- original (3 character context + 200 hidden neurons, 12K params): train 2.058, val 2.105\n",
    "- context: 3 -> 8 (22K params): train 1.918, val 2.027\n",
    "- flat -> hierarchical (22K params): train 1.941, val 2.029\n",
    "- fix bug in batchnorm: train 1.912, val 2.022\n",
    "- scale up the network: n_embd 24, n_hidden 128 (76K params): train 1.769, val 1.993\n",
    "\n",
    "WaveNet-like (76k):\n",
    "train 1.7690281867980957\n",
    "val 1.9936517477035522\n",
    "\n",
    "Original model (75k): train \n",
    "1.7433949708938599\n",
    "val 2.0092387199401855\n",
    "\n",
    "As we can see, scaling up the network provided the greatest improvement in model performance.\n",
    "The true advantage of the hierarchal model used by WaveNet is the ability to use convolutions to *massively* improve training and inference speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nishaya.\n",
      "avella.\n",
      "rayla.\n",
      "lanyiah.\n",
      "pandrick.\n",
      "eliana.\n",
      "illise.\n",
      "akira.\n",
      "ameliami.\n",
      "gatimus.\n",
      "miskell.\n",
      "audaline.\n",
      "amilah.\n",
      "lafi.\n",
      "myra.\n",
      "santza.\n",
      "manassabelle.\n",
      "tathany.\n",
      "shamsyn.\n",
      "denis.\n"
     ]
    }
   ],
   "source": [
    "# sample from the model\n",
    "for _ in range(20):\n",
    "    \n",
    "    out = []\n",
    "    context = [0] * block_size # initialize with all ...\n",
    "    while True:\n",
    "      # forward pass the neural net\n",
    "      logits = model(torch.tensor([context]))\n",
    "      probs = F.softmax(logits, dim=1)\n",
    "      # sample from the distribution\n",
    "      ix = torch.multinomial(probs, num_samples=1).item()\n",
    "      # shift the context window and track the samples\n",
    "      context = context[1:] + [ix]\n",
    "      out.append(ix)\n",
    "      # if we sample the special '.' token, break\n",
    "      if ix == 0:\n",
    "        break\n",
    "    \n",
    "    print(''.join(itos[i] for i in out)) # decode and print the generated word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next time:\n",
    "Why convolutions? Brief preview/hint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........ --> d\n",
      ".......d --> i\n",
      "......di --> o\n",
      ".....dio --> n\n",
      "....dion --> d\n",
      "...diond --> r\n",
      "..diondr --> e\n",
      ".diondre --> .\n"
     ]
    }
   ],
   "source": [
    "for x,y in zip(Xtr[7:15], Ytr[7:15]):\n",
    "  print(''.join(itos[ix.item()] for ix in x), '-->', itos[y.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 27])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward a single example:\n",
    "logits = model(Xtr[[7]])\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 27])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward all of them\n",
    "logits = torch.zeros(8, 27)\n",
    "for i in range(8):\n",
    "  logits[i] = model(Xtr[[7+i]])\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convolution is a \"for loop\"\n",
    "# allows us to forward Linear layers efficiently over space"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
